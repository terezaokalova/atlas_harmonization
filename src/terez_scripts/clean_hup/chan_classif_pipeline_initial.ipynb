{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pycatch22\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "# 1) DATA LOADING & MERGING\n",
    "############################\n",
    "\n",
    "def convert_spared_to_label(val):\n",
    "    \"\"\"\n",
    "    Convert 'spared' to binary: 0 if spared (TRUE), 1 if resected (FALSE).\n",
    "    \"\"\"\n",
    "    if isinstance(val, bool):\n",
    "        return 0 if val else 1\n",
    "    elif isinstance(val, str):\n",
    "        return 0 if val.strip().upper() == 'TRUE' else 1\n",
    "    else:\n",
    "        return 0 if bool(val) else 1\n",
    "\n",
    "def load_all_subjects(subjects_dir, subject_list):\n",
    "    \"\"\"\n",
    "    Loads each subject's aggregated pickle (sub-XXXX_features_averaged.pkl),\n",
    "    adds a 'subject_id' column, and concatenates into one DataFrame.\n",
    "    \"\"\"\n",
    "    all_dfs = []\n",
    "    for subj in subject_list:\n",
    "        subj_path = os.path.join(subjects_dir, subj)\n",
    "        pkl_file = os.path.join(subj_path, f\"{subj}_features_averaged.pkl\")\n",
    "        if not os.path.isfile(pkl_file):\n",
    "            print(f\"Warning: file not found: {pkl_file}\")\n",
    "            continue\n",
    "        df = pd.read_pickle(pkl_file)\n",
    "        df['subject_id'] = subj\n",
    "        if 'spared' not in df.columns:\n",
    "            raise ValueError(f\"'spared' column missing for {subj}\")\n",
    "        df['label'] = df['spared'].apply(convert_spared_to_label)\n",
    "        all_dfs.append(df)\n",
    "    if not all_dfs:\n",
    "        raise ValueError(\"No data loaded. Check paths or subject list.\")\n",
    "    combined_df = pd.concat(all_dfs, axis=0, ignore_index=True)\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# 2) PREPROCESSING\n",
    "######################\n",
    "\n",
    "# dvakrat meraj raz rez\n",
    "def get_explicit_feature_list():\n",
    "    # Bandpower features for each band\n",
    "    band_names = ['delta', 'theta', 'alpha', 'beta', 'gamma']\n",
    "    band_features = [f\"{band}_{metric}\" for band in band_names for metric in ['power', 'rel', 'log']]\n",
    "    \n",
    "    # FOOOF features.\n",
    "    fooof_features = [\n",
    "        'fooof_aperiodic_offset', \n",
    "        'fooof_aperiodic_exponent', \n",
    "        'fooof_r_squared', \n",
    "        'fooof_error', \n",
    "        'fooof_num_peaks'\n",
    "    ]\n",
    "    \n",
    "    # Entropy feature.\n",
    "    entropy_features = ['entropy_5secwin']\n",
    "    \n",
    "    # Obtain catch22 feature names via a dummy call.\n",
    "    dummy = np.random.randn(100).tolist()\n",
    "    res = pycatch22.catch22_all(dummy, catch24=False)\n",
    "    catch22_features = [f\"catch22_{nm}\" for nm in res['names']]\n",
    "    \n",
    "    return band_features + fooof_features + entropy_features + catch22_features\n",
    "\n",
    "# def get_feature_list(df):\n",
    "#     \"\"\"\n",
    "#     Returns a list of feature columns to use.\n",
    "#     We ignore metadata columns like 'label', 'subject_id', and 'spared'.\n",
    "#     \"\"\"\n",
    "#     ignore_cols = {'label', 'subject_id', 'spared'}\n",
    "#     feature_cols = [col for col in df.columns \n",
    "#                     if col not in ignore_cols and pd.api.types.is_numeric_dtype(df[col])]\n",
    "#     return feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# 3) MODEL TRAINING & EVALUATION\n",
    "######################\n",
    "\n",
    "# runs on subjects (not electrodes)\n",
    "def train_and_evaluate_with_groups(X, y, groups, model_choice='random_forest'):\n",
    "    if model_choice == 'random_forest':\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "    elif model_choice == 'logistic':\n",
    "        model = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')\n",
    "    elif model_choice == 'svm':\n",
    "        model = SVC(kernel='rbf', probability=True, random_state=42, class_weight='balanced')\n",
    "    elif model_choice == 'xgboost':\n",
    "        from xgboost import XGBClassifier\n",
    "        # Compute scale_pos_weight to help with imbalance:\n",
    "        pos = np.sum(y == 1)\n",
    "        neg = np.sum(y == 0)\n",
    "        scale_pos_weight = neg / pos if pos > 0 else 1\n",
    "        model = XGBClassifier(scale_pos_weight=scale_pos_weight, random_state=42,\n",
    "                              use_label_encoder=False, eval_metric='logloss')\n",
    "    else:\n",
    "        raise ValueError(\"model_choice must be one of ['random_forest','logistic','svm','xgboost']\")\n",
    "    \n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "    accuracies, aucs = [], []\n",
    "    for train_idx, val_idx in gkf.split(X, y, groups=groups):\n",
    "        X_train_cv, X_val_cv = X[train_idx], X[val_idx]\n",
    "        y_train_cv, y_val_cv = y[train_idx], y[val_idx]\n",
    "        model.fit(X_train_cv, y_train_cv)\n",
    "        y_pred_cv = model.predict(X_val_cv)\n",
    "        acc = accuracy_score(y_val_cv, y_pred_cv)\n",
    "        accuracies.append(acc)\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            y_proba_cv = model.predict_proba(X_val_cv)[:, 1]\n",
    "            aucs.append(roc_auc_score(y_val_cv, y_proba_cv))\n",
    "    print(f\"\\n=== {model_choice.upper()} GROUP-CV Results ===\")\n",
    "    print(f\"Accuracy: {np.mean(accuracies):.3f} ± {np.std(accuracies):.3f}\")\n",
    "    if aucs:\n",
    "        print(f\"ROC AUC:  {np.mean(aucs):.3f} ± {np.std(aucs):.3f}\")\n",
    "    \n",
    "    # Refit on full training set\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "# def train_and_evaluate(X, y, model_choice='random_forest'):\n",
    "#     \"\"\"\n",
    "#     Trains a model with 5-fold cross-validation on (X, y) and returns the fitted model.\n",
    "#     Prints mean accuracy and ROC AUC across folds.\n",
    "#     \"\"\"\n",
    "#     if model_choice == 'random_forest':\n",
    "#         model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "#     elif model_choice == 'logistic':\n",
    "#         model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "#     elif model_choice == 'svm':\n",
    "#         model = SVC(kernel='rbf', probability=True, random_state=42)\n",
    "#     else:\n",
    "#         raise ValueError(\"model_choice must be one of ['random_forest','logistic','svm']\")\n",
    "    \n",
    "#     skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "#     accuracies, aucs = [], []\n",
    "#     for train_idx, val_idx in skf.split(X, y):\n",
    "#         X_train_cv, X_val_cv = X[train_idx], X[val_idx]\n",
    "#         y_train_cv, y_val_cv = y[train_idx], y[val_idx]\n",
    "#         model.fit(X_train_cv, y_train_cv)\n",
    "#         y_pred_cv = model.predict(X_val_cv)\n",
    "#         acc = accuracy_score(y_val_cv, y_pred_cv)\n",
    "#         accuracies.append(acc)\n",
    "#         if hasattr(model, 'predict_proba'):\n",
    "#             y_proba_cv = model.predict_proba(X_val_cv)[:, 1]\n",
    "#             aucs.append(roc_auc_score(y_val_cv, y_proba_cv))\n",
    "    \n",
    "#     print(f\"\\n=== {model_choice.upper()} CROSS-VALIDATION ===\")\n",
    "#     print(f\"Accuracy: {np.mean(accuracies):.3f} ± {np.std(accuracies):.3f}\")\n",
    "#     if aucs:\n",
    "#         print(f\"ROC AUC:  {np.mean(aucs):.3f} ± {np.std(aucs):.3f}\")\n",
    "    \n",
    "#     # Fit on full training data\n",
    "#     model.fit(X, y)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# 4) PIPELINE\n",
    "######################\n",
    "\n",
    "# Adjust these paths and subject list to match your setup.\n",
    "subjects_dir = \"/Users/tereza/nishant/atlas/atlas_work_terez/atlas_harmonization/Data/hup/derivatives/clean\"\n",
    "subject_list = [\n",
    "    \"sub-RID0031\", \"sub-RID0032\", \"sub-RID0033\", \"sub-RID0050\", \"sub-RID0051\", \n",
    "    \"sub-RID0064\", \"sub-RID0089\", \"sub-RID0101\", \"sub-RID0117\", \"sub-RID0143\",\n",
    "    \"sub-RID0167\", \"sub-RID0175\", \"sub-RID0179\", \"sub-RID0238\", \"sub-RID0301\",\n",
    "    \"sub-RID0320\", \"sub-RID0381\", \"sub-RID0405\", \"sub-RID0424\", \"sub-RID0508\",\n",
    "    \"sub-RID0562\", \"sub-RID0589\", \"sub-RID0658\"\n",
    "]\n",
    "\n",
    "print(\"Loading data from all subjects...\")\n",
    "combined_df = load_all_subjects(subjects_dir, subject_list)\n",
    "print(f\"Combined data shape: {combined_df.shape}\")\n",
    "\n",
    "# Use the explicit feature list from the extraction script\n",
    "explicit_feature_list = get_explicit_feature_list()\n",
    "# Only include features that are present in the combined DataFrame\n",
    "present_features = [feat for feat in explicit_feature_list if feat in combined_df.columns]\n",
    "print(f\"Using explicit feature list with {len(present_features)} features.\")\n",
    "\n",
    "# Identify feature columns - prev without explicit feature list\n",
    "# feature_cols = get_feature_list(combined_df)\n",
    "# print(f\"Found {len(feature_cols)} feature columns.\")\n",
    "\n",
    "# Drop rows with missing feature values using the explicit feature list\n",
    "combined_df = combined_df.dropna(subset=present_features)\n",
    "X_full = combined_df[present_features].values\n",
    "y_full = combined_df['label'].values\n",
    "\n",
    "# # Drop rows with missing feature values\n",
    "# combined_df = combined_df.dropna(subset=feature_cols)\n",
    "# X_full = combined_df[feature_cols].values\n",
    "# y_full = combined_df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# 5) TRAIN-TEST SPLIT (80/20) AT SUBJECT LEVEL\n",
    "#############################\n",
    "\n",
    "# Get the unique subject IDs\n",
    "unique_subjects = combined_df['subject_id'].unique()\n",
    "print(\"Total unique subjects:\", len(unique_subjects))\n",
    "print(\"Unique subjects:\", unique_subjects)\n",
    "\n",
    "# Split subjects into training and testing sets (80/20 split)\n",
    "train_subjects, test_subjects = train_test_split(unique_subjects, test_size=0.2, random_state=42)\n",
    "print(\"Number of training subjects:\", len(train_subjects))\n",
    "print(\"Training subjects:\", train_subjects)\n",
    "print(\"Number of testing subjects:\", len(test_subjects))\n",
    "print(\"Testing subjects:\", test_subjects)\n",
    "\n",
    "# Create masks to select rows for train and test based on subject_id\n",
    "train_mask = combined_df['subject_id'].isin(train_subjects)\n",
    "test_mask = combined_df['subject_id'].isin(test_subjects)\n",
    "\n",
    "# Use the explicit feature list variable 'present_features'\n",
    "X_train = combined_df[train_mask][present_features].values\n",
    "y_train = combined_df[train_mask]['label'].values\n",
    "\n",
    "X_test = combined_df[test_mask][present_features].values\n",
    "y_test = combined_df[test_mask]['label'].values\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "\n",
    "# Standardize features based on the training set\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# # Here, we perform a random 80/20 split at the electrode level.\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X_full, y_full, test_size=0.2, stratify=y_full, random_state=42\n",
    "# )\n",
    "\n",
    "# # Standardize features based on the training set\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# 6) TRAIN 3 MODELS & EVALUATE\n",
    "#############################\n",
    "\n",
    "# models_to_run = ['random_forest', 'logistic', 'svm', 'xgboost']\n",
    "models_to_run = ['xgboost']\n",
    "trained_models = {}\n",
    "\n",
    "for model_name in models_to_run:\n",
    "    print(\"\\n=======================================\")\n",
    "    print(f\"Training Model: {model_name.upper()}\")\n",
    "    \n",
    "    # Create the group vector for training (subject IDs corresponding to each electrode)\n",
    "    train_groups = combined_df[train_mask]['subject_id'].values\n",
    "    \n",
    "    # Train the model using group-aware cross-validation\n",
    "    clf = train_and_evaluate_with_groups(X_train_scaled, y_train, groups=train_groups, model_choice=model_name)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    y_pred_test = clf.predict(X_test_scaled)\n",
    "    print(f\"\\n--- {model_name.upper()} TEST SET PERFORMANCE ---\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred_test))\n",
    "    if hasattr(clf, 'predict_proba'):\n",
    "        y_proba_test = clf.predict_proba(X_test_scaled)[:, 1]\n",
    "        print(\"ROC AUC:\", roc_auc_score(y_test, y_proba_test))\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred_test))\n",
    "    \n",
    "    # Display confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred_test)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot(values_format='d')\n",
    "    plt.title(f\"Confusion Matrix - {model_name.upper()}\")\n",
    "    plt.show()\n",
    "    \n",
    "    trained_models[model_name] = clf\n",
    "\n",
    "# for model_name in models_to_run:\n",
    "#     print(\"\\n=======================================\")\n",
    "#     print(f\"Training Model: {model_name.upper()}\")\n",
    "#     clf = train_and_evaluate(X_train_scaled, y_train, model_choice=model_name)\n",
    "    \n",
    "#     # Evaluate on test set\n",
    "#     y_pred_test = clf.predict(X_test_scaled)\n",
    "#     print(f\"\\n--- {model_name.upper()} TEST SET PERFORMANCE ---\")\n",
    "#     print(\"Accuracy:\", accuracy_score(y_test, y_pred_test))\n",
    "#     if hasattr(clf, 'predict_proba'):\n",
    "#         y_proba_test = clf.predict_proba(X_test_scaled)[:, 1]\n",
    "#         print(\"ROC AUC:\", roc_auc_score(y_test, y_proba_test))\n",
    "#     print(\"\\nClassification Report:\")\n",
    "#     print(classification_report(y_test, y_pred_test))\n",
    "    \n",
    "#     # Display confusion matrix\n",
    "#     cm = confusion_matrix(y_test, y_pred_test)\n",
    "#     disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "#     disp.plot(values_format='d')\n",
    "#     plt.title(f\"Confusion Matrix - {model_name.upper()}\")\n",
    "#     plt.show()\n",
    "    \n",
    "#     trained_models[model_name] = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# 7) FEATURE IMPORTANCE ASSESSMENT\n",
    "#############################\n",
    "\n",
    "print(\"\\n=== FEATURE IMPORTANCE ASSESSMENT ===\")\n",
    "\n",
    "# (a) Random Forest built-in importances\n",
    "rf_model = trained_models.get('random_forest')\n",
    "if rf_model is not None and hasattr(rf_model, 'feature_importances_'):\n",
    "    importances = rf_model.feature_importances_\n",
    "    sorted_idx = np.argsort(importances)[::-1]\n",
    "    sorted_features = [present_features[i] for i in sorted_idx]\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.bar(range(len(importances)), importances[sorted_idx], align='center')\n",
    "    plt.xticks(range(len(importances)), sorted_features, rotation=90)\n",
    "    plt.title(\"Random Forest Feature Importances\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"Top 10 RF Features:\")\n",
    "    for i in range(min(10, len(sorted_features))):\n",
    "        print(f\"{i+1}. {sorted_features[i]} (importance={importances[sorted_idx][i]:.4f})\")\n",
    "\n",
    "# (b) Permutation Importance for Logistic Regression\n",
    "lr_model = trained_models.get('logistic')\n",
    "if lr_model is not None:\n",
    "    print(\"\\nPermutation Importance for Logistic Regression (Test Set):\")\n",
    "    result = permutation_importance(\n",
    "        lr_model, X_test_scaled, y_test, n_repeats=20, random_state=42, scoring='roc_auc'\n",
    "    )\n",
    "    perm_importances = result.importances_mean\n",
    "    sorted_idx = np.argsort(perm_importances)[::-1]\n",
    "    sorted_features = [present_features[i] for i in sorted_idx]\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.bar(range(len(perm_importances)), perm_importances[sorted_idx], align='center')\n",
    "    plt.xticks(range(len(perm_importances)), sorted_features, rotation=90)\n",
    "    plt.title(\"Permutation Importance (Logistic Regression)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# (c) Permutation Importance for SVM (since SHAP is avoided)\n",
    "svm_model = trained_models.get('svm')\n",
    "if svm_model is not None:\n",
    "    print(\"\\nPermutation Importance for SVM (Test Set):\")\n",
    "    result = permutation_importance(\n",
    "        svm_model, X_test_scaled, y_test, n_repeats=20, random_state=42, scoring='roc_auc'\n",
    "    )\n",
    "    perm_importances = result.importances_mean\n",
    "    sorted_idx = np.argsort(perm_importances)[::-1]\n",
    "    sorted_features = [present_features[i] for i in sorted_idx]\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.bar(range(len(perm_importances)), perm_importances[sorted_idx], align='center')\n",
    "    plt.xticks(range(len(perm_importances)), sorted_features, rotation=90)\n",
    "    plt.title(\"Permutation Importance (SVM)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "print(\"=== Detailed Summary of Model Results on Test Set ===\")\n",
    "\n",
    "# Loop over each trained model and compute metrics\n",
    "for model_name in models_to_run:\n",
    "    print(\"\\n------------------------------\")\n",
    "    print(f\"Model: {model_name.upper()}\")\n",
    "    \n",
    "    model = trained_models[model_name]\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Compute ROC AUC if the model supports predict_proba\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "        roc_auc = roc_auc_score(y_test, y_proba)\n",
    "    else:\n",
    "        roc_auc = \"N/A\"\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Print metrics and classification report; set zero_division=0 to avoid warnings\n",
    "    print(\"Accuracy: {:.3f}\".format(accuracy))\n",
    "    print(\"ROC AUC: {}\".format(roc_auc))\n",
    "    clf_report = classification_report(y_test, y_pred, zero_division=0)\n",
    "    print(\"Classification Report:\")\n",
    "    print(clf_report)\n",
    "    \n",
    "    # Display the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    # disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    # disp.plot(values_format='d')\n",
    "    # plt.title(f\"Confusion Matrix - {model_name.upper()}\")\n",
    "    # plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnt_gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
