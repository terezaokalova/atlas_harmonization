{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/tereza/nishant/atlas/atlas_work_terez/atlas_harmonization/external_dependencies/CNT_research_tools/python')\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from CNTtools import settings\n",
    "from CNTtools import iEEGPreprocess  # Direct import from CNTtools\n",
    "\n",
    "# To access iEEGData class\n",
    "from CNTtools import iEEGData  # If this is a direct import\n",
    "# OR\n",
    "from CNTtools.iEEGPreprocess import iEEGData  # If iEEGData is in the iEEGPreprocess module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'iEEGData', 'iEEGPreprocess', 'settings', 'tools']\n"
     ]
    }
   ],
   "source": [
    "import CNTtools\n",
    "print(dir(CNTtools))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In CNTtools/tools.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def login_config():\n",
    "    # Prompt user for login info and create a config file.\n",
    "    username = input(\"Enter username: \")\n",
    "    # Code to write configuration files would go here.\n",
    "    return username\n",
    "\n",
    "def get_ieeg_data(username, password, filename, start, stop, select_elecs, ignore_elecs):\n",
    "    # Connect to ieeg.org and download data.\n",
    "    # For demonstration, generate dummy data.\n",
    "    fs = 1000.0\n",
    "    n_samples = int((stop - start) * fs)\n",
    "    n_channels = 16  # dummy number\n",
    "    data = np.random.randn(n_samples, n_channels)\n",
    "    ch_names = np.array([f\"Ch{i}\" for i in range(n_channels)])\n",
    "    return data, fs, ch_names\n",
    "\n",
    "def clean_labels(ch_names):\n",
    "    # Standardize channel labels (e.g., strip whitespace, convert to upper case).\n",
    "    return np.array([name.strip().upper() for name in ch_names])\n",
    "\n",
    "def find_non_ieeg(ch_names):\n",
    "    # Identify non-iEEG channels. Dummy: mark none as non-iEEG.\n",
    "    return np.array([False] * len(ch_names))\n",
    "\n",
    "def identify_bad_chs(data, fs):\n",
    "    # Identify bad channels based on a simple criterion (e.g., low variance).\n",
    "    bad = np.std(data, axis=0) < 0.5\n",
    "    reject_details = {i: \"Low variance\" for i, b in enumerate(bad) if b}\n",
    "    return bad, reject_details\n",
    "\n",
    "def bandpass_filter(data, fs, low_freq, high_freq):\n",
    "    # Apply a bandpass filter. This dummy returns the data unchanged.\n",
    "    return data\n",
    "\n",
    "def notch_filter(data, fs, notch_freq):\n",
    "    # Apply a notch filter. This dummy returns the data unchanged.\n",
    "    return data\n",
    "\n",
    "def car(data, ch_names):\n",
    "    # Common Average Reference: subtract the average across channels.\n",
    "    avg = np.mean(data, axis=1, keepdims=True)\n",
    "    new_data = data - avg\n",
    "    ref_chnames = np.array(ch_names)  # unchanged in this dummy\n",
    "    return new_data, ref_chnames\n",
    "\n",
    "def bipolar(data, ch_names):\n",
    "    # Bipolar re-referencing: compute differences between adjacent channels.\n",
    "    new_data = np.diff(data, axis=1)\n",
    "    ref_chnames = np.array([f\"{ch_names[i]}-{ch_names[i+1]}\" for i in range(len(ch_names)-1)])\n",
    "    return new_data, ref_chnames\n",
    "\n",
    "def get_elec_locs(filename, ch_names, loc_file):\n",
    "    # Load electrode locations from a CSV file.\n",
    "    locs_df = pd.read_csv(loc_file)\n",
    "    # Match electrode names; this is a dummy implementation.\n",
    "    locs = {name: locs_df[locs_df['electrodeName'] == name].iloc[0].to_dict() for name in ch_names}\n",
    "    return locs\n",
    "\n",
    "def laplacian(data, ch_names, locs, radius):\n",
    "    # Apply Laplacian referencing. Dummy: return data unchanged.\n",
    "    ref_chnames = np.array(ch_names)\n",
    "    return data, ref_chnames\n",
    "\n",
    "def pre_whiten(data):\n",
    "    # Pre-whitening: remove autocorrelation. Dummy implementation.\n",
    "    return data\n",
    "\n",
    "def bandpower(data, fs, band, window, relative):\n",
    "    # Compute band power in a frequency band. Dummy: return a random value.\n",
    "    return np.random.rand()\n",
    "\n",
    "def line_length(data):\n",
    "    # Compute the line length (sum of absolute differences) for each channel.\n",
    "    return np.sum(np.abs(np.diff(data, axis=0)), axis=0)\n",
    "\n",
    "def pearson(data, fs, win, win_size):\n",
    "    # Compute Pearson correlation across channels.\n",
    "    return np.corrcoef(data.T)\n",
    "\n",
    "def squared_pearson(data, fs, win, win_size):\n",
    "    corr = np.corrcoef(data.T)\n",
    "    return corr ** 2\n",
    "\n",
    "def cross_correlation(data, fs, win, win_size):\n",
    "    # Compute cross-correlation between channels.\n",
    "    nchs = data.shape[1]\n",
    "    corr = np.zeros((nchs, nchs))\n",
    "    for i in range(nchs):\n",
    "        for j in range(nchs):\n",
    "            corr[i, j] = np.correlate(data[:, i], data[:, j])[0]\n",
    "    return corr, None\n",
    "\n",
    "def coherence(data, fs, win, win_size, segment, overlap):\n",
    "    # Compute coherence. Dummy: return a random matrix.\n",
    "    nchs = data.shape[1]\n",
    "    return np.random.rand(nchs, nchs)\n",
    "\n",
    "def plv(data, fs, win, win_size):\n",
    "    # Compute Phase Locking Value. Dummy: return a random matrix.\n",
    "    nchs = data.shape[1]\n",
    "    return np.random.rand(nchs, nchs)\n",
    "\n",
    "def relative_entropy(data, fs, win, win_size):\n",
    "    # Compute relative entropy. Dummy: return a random matrix.\n",
    "    nchs = data.shape[1]\n",
    "    return np.random.rand(nchs, nchs)\n",
    "\n",
    "def plot_ieeg_data(data, ch_names, t):\n",
    "    # Plot the iEEG data.\n",
    "    fig, ax = plt.subplots()\n",
    "    for i in range(data.shape[1]):\n",
    "        ax.plot(t, data[:, i] + i * 5)  # Offset each channel for clarity.\n",
    "    ax.set_xlabel(\"Time (s)\")\n",
    "    ax.set_ylabel(\"Amplitude\")\n",
    "    ax.set_title(\"iEEG Data Plot\")\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-RID0031: 81 electrodes\n",
      "sub-RID0032: 29 electrodes\n",
      "sub-RID0033: 90 electrodes\n",
      "sub-RID0050: 32 electrodes\n",
      "sub-RID0051: 61 electrodes\n",
      "sub-RID0064: 102 electrodes\n",
      "sub-RID0089: 104 electrodes\n",
      "sub-RID0101: 45 electrodes\n",
      "sub-RID0117: 95 electrodes\n",
      "sub-RID0143: 88 electrodes\n",
      "sub-RID0167: 69 electrodes\n",
      "sub-RID0175: 8 electrodes\n",
      "sub-RID0179: 82 electrodes\n",
      "sub-RID0238: 78 electrodes\n",
      "sub-RID0301: 67 electrodes\n",
      "sub-RID0320: 58 electrodes\n",
      "sub-RID0381: 75 electrodes\n",
      "sub-RID0405: 102 electrodes\n",
      "sub-RID0424: 122 electrodes\n",
      "sub-RID0508: 47 electrodes\n",
      "sub-RID0562: 31 electrodes\n",
      "sub-RID0589: 83 electrodes\n",
      "sub-RID0658: 126 electrodes\n",
      "             Electrode Count\n",
      "sub-RID0031               81\n",
      "sub-RID0032               29\n",
      "sub-RID0033               90\n",
      "sub-RID0050               32\n",
      "sub-RID0051               61\n",
      "sub-RID0064              102\n",
      "sub-RID0089              104\n",
      "sub-RID0101               45\n",
      "sub-RID0117               95\n",
      "sub-RID0143               88\n",
      "sub-RID0167               69\n",
      "sub-RID0175                8\n",
      "sub-RID0179               82\n",
      "sub-RID0238               78\n",
      "sub-RID0301               67\n",
      "sub-RID0320               58\n",
      "sub-RID0381               75\n",
      "sub-RID0405              102\n",
      "sub-RID0424              122\n",
      "sub-RID0508               47\n",
      "sub-RID0562               31\n",
      "sub-RID0589               83\n",
      "sub-RID0658              126\n"
     ]
    }
   ],
   "source": [
    "def get_clean_hup_file_paths(base_path):\n",
    "    subjects_dict = {}\n",
    "    subject_dirs = sorted([d for d in os.listdir(base_path) \n",
    "                           if os.path.isdir(os.path.join(base_path, d)) and d.startswith(\"sub-\")])\n",
    "    for subject in subject_dirs:\n",
    "        subject_path = os.path.join(base_path, subject)\n",
    "        pkl_files = sorted([f for f in os.listdir(subject_path)\n",
    "                            if f.startswith(\"interictal_eeg_bipolar_clean_\") and f.endswith('.pkl')])\n",
    "        if len(pkl_files) != 20:\n",
    "            print(f\"WARNING: {subject} has {len(pkl_files)} files instead of 20.\")\n",
    "        subjects_dict[subject] = {}\n",
    "        for idx, filename in enumerate(pkl_files, start=1):\n",
    "            file_path = os.path.join(subject_path, filename)\n",
    "            subjects_dict[subject][idx] = file_path\n",
    "    return subjects_dict\n",
    "\n",
    "def load_epoch(file_path):\n",
    "    obj = pd.read_pickle(file_path)\n",
    "    if isinstance(obj, dict):\n",
    "        df = obj.get('metadata').copy()\n",
    "        data_obj = obj.get('data')\n",
    "        if isinstance(data_obj, pd.DataFrame):\n",
    "            if data_obj.shape[0] > data_obj.shape[1]:\n",
    "                data_obj = data_obj.T\n",
    "            df['data'] = data_obj.apply(lambda row: row.values, axis=1)\n",
    "        else:\n",
    "            df['data'] = [np.asarray(x) for x in data_obj]\n",
    "        return df\n",
    "    return obj\n",
    "\n",
    "base_path = \"/Users/tereza/nishant/atlas/atlas_work_terez/atlas_harmonization/Data/hup/derivatives/clean\"\n",
    "\n",
    "# Get file paths for each subject.\n",
    "subject_files = get_clean_hup_file_paths(base_path)\n",
    "\n",
    "# Dictionary to store the electrode counts.\n",
    "subject_electrode_counts = {}\n",
    "\n",
    "# For each subject, load the first epoch and count electrodes.\n",
    "for subject, epochs in subject_files.items():\n",
    "    if 1 in epochs:\n",
    "        df = load_epoch(epochs[1])\n",
    "        # Assuming each row in df corresponds to one electrode.\n",
    "        electrode_count = df.shape[0]\n",
    "        subject_electrode_counts[subject] = electrode_count\n",
    "        print(f\"{subject}: {electrode_count} electrodes\")\n",
    "    else:\n",
    "        print(f\"{subject}: No epoch 1 found.\")\n",
    "\n",
    "results_df = pd.DataFrame.from_dict(subject_electrode_counts, orient='index', columns=['Electrode Count'])\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-correlation shape: (81, 81)\n",
      "Coherence shape: (81, 81, 7)\n",
      "Pearson correlation shape: (81, 81)\n",
      "Line length: [ 4.82027033  2.38358265  1.75585806  2.4995317   3.16329884  5.91096975\n",
      " 12.50842765  8.87184142  4.28047809  2.5909629   2.32105354  2.09084481\n",
      "  2.26818624  2.38404131  2.87973314  2.39244793  4.46663646 12.46826165\n",
      "  6.17056843  2.75519093  1.76225699  1.46379304  1.83400456  4.45243274\n",
      "  4.32839807  2.12048065  7.56739053  9.12891943  5.96646244  3.05332233\n",
      "  1.38949696 10.92783461 10.43728509  8.86625762  5.40695825  3.25816594\n",
      "  3.22013808  2.7233554   4.30113789  3.07454238  5.92775183 14.6232645\n",
      " 14.35938695  4.13933805  3.29334034  5.96492859  3.74507173  2.70169612\n",
      "  9.10757212  4.78196428  5.9187558   3.42845945  4.76673177  6.96830275\n",
      "  7.74207147  7.99881656 17.71962455 18.45744645 17.56824361  3.82787304\n",
      "  5.77206987  8.54549252  4.99019639  3.42664813  4.14815821  4.26452196\n",
      "  4.25385373  4.9443768   3.13908282  2.75740188  2.7844904   2.97315544\n",
      "  3.37875696  6.00992429  2.12719403  2.62548195  5.74674319  3.45848476\n",
      "  5.23583628 11.62871659  4.17030449]\n"
     ]
    }
   ],
   "source": [
    "# Functions to get file paths and load an epoch (as provided)\n",
    "def get_clean_hup_file_paths(base_path):\n",
    "    subjects_dict = {}\n",
    "    subject_dirs = sorted([d for d in os.listdir(base_path) \n",
    "                           if os.path.isdir(os.path.join(base_path, d)) and d.startswith(\"sub-\")])\n",
    "    for subject in subject_dirs:\n",
    "        subject_path = os.path.join(base_path, subject)\n",
    "        pkl_files = sorted([f for f in os.listdir(subject_path)\n",
    "                            if f.startswith(\"interictal_eeg_bipolar_clean_\") and f.endswith('.pkl')])\n",
    "        if len(pkl_files) != 20:\n",
    "            print(f\"WARNING: {subject} has {len(pkl_files)} files instead of 20.\")\n",
    "        subjects_dict[subject] = {}\n",
    "        for idx, filename in enumerate(pkl_files, start=1):\n",
    "            file_path = os.path.join(subject_path, filename)\n",
    "            subjects_dict[subject][idx] = file_path\n",
    "    return subjects_dict\n",
    "\n",
    "def load_epoch(file_path):\n",
    "    obj = pd.read_pickle(file_path)\n",
    "    if isinstance(obj, dict):\n",
    "        df = obj.get('metadata').copy()\n",
    "        data_obj = obj.get('data')\n",
    "        if isinstance(data_obj, pd.DataFrame):\n",
    "            if data_obj.shape[0] > data_obj.shape[1]:\n",
    "                data_obj = data_obj.T\n",
    "            df['data'] = data_obj.apply(lambda row: row.values, axis=1)\n",
    "        else:\n",
    "            df['data'] = [np.asarray(x) for x in data_obj]\n",
    "        return df\n",
    "    return obj\n",
    "\n",
    "base_path = \"/Users/tereza/nishant/atlas/atlas_work_terez/atlas_harmonization/Data/hup/derivatives/clean\"\n",
    "file_paths = get_clean_hup_file_paths(base_path)\n",
    "subject = list(file_paths.keys())[0]           # choose first subject\n",
    "epoch_key = list(file_paths[subject].keys())[0]  # choose first epoch\n",
    "epoch_file = file_paths[subject][epoch_key]\n",
    "\n",
    "# Load the epoch data\n",
    "df = load_epoch(epoch_file)\n",
    "\n",
    "# Stack each electrode's time series (each row in df['data']) into a 2D array:\n",
    "# data shape: (time, channels)\n",
    "data = np.column_stack(df['data'].values)\n",
    "ch_names = df.index.values  # assuming the DataFrame index has electrode labels\n",
    "\n",
    "# Define sampling frequency and time bounds\n",
    "fs = 1000.0  # Hz (adjust if needed)\n",
    "start = 0\n",
    "stop = data.shape[0] / fs  # duration in seconds\n",
    "\n",
    "# Create an iEEGData instance; note that filename is arbitrary here\n",
    "ieeg = iEEGData(filename=\"example_epoch\", start=start, stop=stop, data=data, fs=fs, ch_names=ch_names)\n",
    "\n",
    "# Compute cross-correlation (windowed with 2-sec window)\n",
    "ieeg.cross_corr(win=True, win_size=2)\n",
    "\n",
    "# Compute coherence (using a 2-sec window, 1-sec segments with 0.5-sec overlap)\n",
    "ieeg.coherence(win=True, win_size=2, segment=1, overlap=0.5)\n",
    "\n",
    "# Compute Pearson correlation (windowed)\n",
    "ieeg.pearson(win=True, win_size=2)\n",
    "\n",
    "# Compute line length\n",
    "ll = ieeg.line_length()\n",
    "\n",
    "# Print out some results\n",
    "print(\"Cross-correlation shape:\", ieeg.conn[\"cross_corr\"].shape)\n",
    "print(\"Coherence shape:\", ieeg.conn[\"coh\"].shape)\n",
    "print(\"Pearson correlation shape:\", ieeg.conn[\"pearson\"].shape)\n",
    "print(\"Line length:\", ll)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.column_stack(df['data'].values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnt_gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
