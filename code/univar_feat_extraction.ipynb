{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import joblib \n",
    "import dill \n",
    "\n",
    "import mne\n",
    "from scipy.signal import welch, get_window\n",
    "from scipy.signal.windows import hamming\n",
    "\n",
    "from scipy.signal import butter, lfilter, sosfilt, filtfilt, sosfreqz, iirnotch\n",
    "from scipy import fftpack\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path_data = '../Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TIME SERIES DATA\n",
    "\n",
    "# dict_keys(['__header__', '__version__', '__globals__', 'SamplingFrequency', 'depth_elecs', 'mni_coords', 'patient_no', 'resected_ch', 'soz_ch', 'spike_24h', 'wake_clip'])\n",
    "hup_atlas = sio.loadmat(os.path.join(base_path_data, 'HUP_atlas.mat'))\n",
    "# dict_keys(['__header__', '__version__', '__globals__', 'AgeAtTimeOfStudy', 'ChannelName', 'ChannelPosition', 'ChannelRegion', 'ChannelType', \n",
    "# 'Data_N2', 'Data_N3', 'Data_R', 'Data_W', 'FacesLeft', 'FacesRight', 'Gender', 'Hemisphere', 'NodesLeft', 'NodesLeftInflated', 'NodesRegionLeft', \n",
    "# 'NodesRegionRight', 'NodesRight', 'NodesRightInflated', 'Patient', 'RegionName', 'SamplingFrequency'])\n",
    "mni_atlas = sio.loadmat(os.path.join(base_path_data, 'MNI_atlas.mat'))\n",
    "\n",
    "# (12000, 3431) @ x-axis is time steps, y-axis is electrodes\n",
    "hup_ts = pd.DataFrame(hup_atlas['wake_clip'])\n",
    "# (13600, 1765)  @ x-axis is time steps, y-axis is electrodes\n",
    "mni_ts = pd.DataFrame(mni_atlas['Data_W'])\n",
    "\n",
    "# accessing columns for patients\n",
    "hup_patients = pd.DataFrame(hup_atlas['patient_no'])\n",
    "mni_patients = pd.DataFrame(mni_atlas['Patient'])\n",
    "\n",
    "# electrode counts\n",
    "hup_patient_total_el_counts = len(hup_atlas['patient_no'])\n",
    "mni_patient_total_el_counts = len(mni_atlas['Patient'])\n",
    "\n",
    "# unique patient ids\n",
    "hup_patient_ids = np.unique(hup_atlas['patient_no'])\n",
    "mni_patient_ids = np.unique(mni_atlas['Patient']) \n",
    "\n",
    "# sampling frequency\n",
    "mni_samp_freq = int(mni_atlas['SamplingFrequency'].flatten()[~np.isnan(mni_atlas['SamplingFrequency'].flatten())][0])\n",
    "hup_samp_freq = int(hup_atlas['SamplingFrequency'].flatten()[~np.isnan(hup_atlas['SamplingFrequency'].flatten())][0])\n",
    "\n",
    "# mapping electrodes to their respective patients\n",
    "hup_patient_numbers = hup_atlas['patient_no'].flatten()\n",
    "hup_el_to_pat_map_dict = {}\n",
    "for idx, patient_num in enumerate(hup_patient_numbers):\n",
    "    hup_el_to_pat_map_dict[idx] = patient_num\n",
    "hup_idx_map_arr = np.array([patient_num for patient_num in hup_patient_numbers]) # arr equivalent\n",
    "\n",
    "mni_patient_numbers = mni_atlas['Patient'].flatten()\n",
    "mni_el_to_pat_map_dict = {}\n",
    "for idx, patient_num in enumerate(mni_patient_numbers):\n",
    "    mni_el_to_pat_map_dict[idx] = patient_num\n",
    "mni_idx_map_arr = np.array([patient_num for patient_num in mni_patient_numbers])\n",
    "\n",
    "## REGION MAPS\n",
    "\n",
    "dk_atlas_df = pd.read_csv(os.path.join(base_path_data, 'desikanKilliany.csv'))\n",
    "# columns: Index(['x', 'y', 'z', 'roiNum', 'snum', 'abvr', 'lobe', 'isSideLeft'], dtype='object')\n",
    "hup_df = pd.read_csv(os.path.join(base_path_data, 'hup_df.csv'))\n",
    "# columns: Index(['x', 'y', 'z', 'roiNum', 'snum', 'abvr', 'lobe', 'isSideLeft'], dtype='object')\n",
    "mni_df = pd.read_csv(os.path.join(base_path_data, 'mni_df.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norm_psd(iEEGnormal, data_timeS, sampling_frequency=200):\n",
    "    \"\"\"\n",
    "    Function to compute normalized power spectral densities for different EEG frequency bands.\n",
    "    \n",
    "    Args:\n",
    "    iEEGnormal (DataFrame): A DataFrame to append results to.\n",
    "    data_timeS (array): Time domain EEG data for a single electrode (1D array)\n",
    "    sampling_frequency (int): Sampling frequency of the EEG data.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: Updated DataFrame with new EEG features.\n",
    "    \"\"\"\n",
    "    \n",
    "    Fs = sampling_frequency\n",
    "    window = Fs * 2\n",
    "    NFFT = window\n",
    "    \n",
    "    # Compute PSD\n",
    "    f, data_psd = welch(data_timeS, fs=Fs, window=hamming(window), \n",
    "                       nfft=NFFT, scaling='density', noverlap=window//2)\n",
    "    \n",
    "    # filter out noise frequency 57.5Hz to 62.5Hz\n",
    "    noise_mask = (f >= 57.5) & (f <= 62.5)\n",
    "    f = f[~noise_mask]\n",
    "    # Handle 1D data_psd\n",
    "    data_psd = data_psd[~noise_mask]\n",
    "    \n",
    "    def bandpower(psd, freqs, freq_range):\n",
    "        \"\"\"Calculate power in the given frequency range.\"\"\"\n",
    "        idx = np.logical_and(freqs >= freq_range[0], freqs <= freq_range[1])\n",
    "        return np.trapezoid(psd[idx], freqs[idx]) \n",
    "    \n",
    "    # Define frequency bands\n",
    "    bands = {'delta': (1, 4), 'theta': (4, 8), 'alpha': (8, 13), \n",
    "             'beta': (13, 30), 'gamma': (30, 80), 'broad': (1, 80)}\n",
    "    \n",
    "    # Calculate band powers (using 1D data_psd)\n",
    "    band_powers = {band: bandpower(data_psd, f, freq_range) \n",
    "                  for band, freq_range in bands.items()}\n",
    "    \n",
    "    # Compute log transform\n",
    "    log_band_powers = {f'{band}log': np.log10(power + 1) \n",
    "                      for band, power in band_powers.items()}\n",
    "    \n",
    "    # Calculate total power\n",
    "    total_band_power = np.sum([value for value in log_band_powers.values()])\n",
    "    \n",
    "    # Calculate relative powers\n",
    "    relative_band_powers = {f'{band}Rel': log_band_powers[f'{band}log'] / total_band_power \n",
    "                          for band in bands}\n",
    "    \n",
    "    # Create DataFrame row\n",
    "    data_to_append = pd.DataFrame([relative_band_powers])\n",
    "    # data_to_append['broadlog'] = log_band_powers['broadlog']\n",
    "    \n",
    "    # Append to existing DataFrame\n",
    "    iEEGnormal = pd.concat([iEEGnormal, data_to_append], ignore_index=True)\n",
    "    \n",
    "    return iEEGnormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not actually used\n",
    "def compute_shannon_entropy(signal):\n",
    "    \"\"\"\n",
    "    Compute Shannon entropy from EEG signal\n",
    "    \n",
    "    Parameters:\n",
    "    signal: 1D array of EEG values\n",
    "    \n",
    "    Returns:\n",
    "    float: Shannon entropy value\n",
    "    \"\"\"\n",
    "    # 1. Estimate probability distribution using histogram\n",
    "    hist, bin_edges = np.histogram(signal, bins='auto', density=True)\n",
    "    \n",
    "    # 2. Normalize to ensure probabilities sum to 1\n",
    "    probabilities = hist / hist.sum()\n",
    "    \n",
    "    # 3. Remove any zero probabilities (since log(0) is undefined)\n",
    "    probabilities = probabilities[probabilities > 0]\n",
    "    \n",
    "    # 4. Compute Shannon entropy\n",
    "    H = -np.sum(probabilities * np.log2(probabilities))\n",
    "    \n",
    "    # 5. Log transform (optional, for feature scaling)\n",
    "    H_log = np.log10(H + 1)\n",
    "    \n",
    "    return H_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norm_entropy_full_ts(ieeg_normal, data_time_s, sampling_frequency=200, window_size_mins=1, stride_mins=0.5):\n",
    "    if data_time_s.ndim == 1:\n",
    "        data_time_s = data_time_s.reshape(-1,1)\n",
    "    \n",
    "    # Calculate window parameters\n",
    "    samples_per_window = sampling_frequency * 60 * window_size_mins\n",
    "    stride_samples = sampling_frequency * 60 * stride_mins\n",
    "    n_windows = int((len(data_time_s) - samples_per_window) // stride_samples + 1)\n",
    "    \n",
    "    entropies = []\n",
    "    for i in range(n_windows):\n",
    "        start_idx = int(i * stride_samples)\n",
    "        end_idx = int(start_idx + samples_per_window)\n",
    "        window_data = data_time_s[start_idx:end_idx, :]\n",
    "        \n",
    "        # Apply filters\n",
    "        b, a = butter(3, 80/(sampling_frequency/2), btype='low')\n",
    "        filtered = filtfilt(b, a, window_data.astype(float), axis=0)\n",
    "        \n",
    "        b, a = butter(3, 1/(sampling_frequency/2), btype='high')\n",
    "        filtered = filtfilt(b, a, filtered, axis=0)\n",
    "        \n",
    "        b, a = iirnotch(60, 30, sampling_frequency)\n",
    "        filtered = filtfilt(b, a, filtered, axis=0)\n",
    "        \n",
    "        # Compute entropy for this window\n",
    "        signal = filtered[:, 0]\n",
    "        \n",
    "        # 1. Estimate probability distribution\n",
    "        hist, _ = np.histogram(signal, bins='auto', density=True)\n",
    "        \n",
    "        # 2. Normalize probabilities\n",
    "        probabilities = hist / hist.sum()\n",
    "        \n",
    "        # 3. Remove zeros\n",
    "        probabilities = probabilities[probabilities > 0]\n",
    "        \n",
    "        # 4. Compute Shannon entropy\n",
    "        H = -np.sum(probabilities * np.log2(probabilities))\n",
    "        \n",
    "        entropies.append(H)\n",
    "    \n",
    "    # Compute statistics across windows\n",
    "    mean_entropy = np.mean(entropies)\n",
    "    std_entropy = np.std(entropies)\n",
    "    \n",
    "    # 5. Log transform of mean entropy\n",
    "    mean_entropy_log = np.log10(mean_entropy + 1)\n",
    "    \n",
    "    # Create features\n",
    "    data_to_append = pd.DataFrame({\n",
    "        'entropy_mean': [mean_entropy_log],\n",
    "        'entropy_std': [std_entropy]\n",
    "    })\n",
    "    \n",
    "    return pd.concat([ieeg_normal, data_to_append], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norm_entropy_1min_seg(ieeg_normal, data_time_s, sampling_frequency=200):\n",
    "    if data_time_s.ndim == 1:\n",
    "        data_time_s = data_time_s.reshape(-1,1)\n",
    "    \n",
    "    # Get first minute of data\n",
    "    data_seg = data_time_s[:sampling_frequency*60, :]\n",
    "    \n",
    "    # Low pass filter at 80Hz\n",
    "    b, a = butter(3, 80/(sampling_frequency/2), btype='low')\n",
    "    data_seg_filtered = filtfilt(b, a, data_seg.astype(float), axis=0)\n",
    "    \n",
    "    # High pass filter at 1Hz  \n",
    "    b, a = butter(3, 1/(sampling_frequency/2), btype='high')\n",
    "    data_seg_filtered = filtfilt(b, a, data_seg_filtered, axis=0)\n",
    "    \n",
    "    # Notch filter at 60Hz\n",
    "    b, a = iirnotch(60, 30, sampling_frequency)\n",
    "    data_seg_notch = filtfilt(b, a, data_seg_filtered, axis=0)\n",
    "\n",
    "    # Compute Shannon entropy for each channel\n",
    "    data_entropy = np.zeros((data_seg_notch.shape[1], 1))\n",
    "    for chan in range(data_seg_notch.shape[1]):\n",
    "        signal = data_seg_notch[:, chan]\n",
    "        # Estimate probability distribution\n",
    "        hist, _ = np.histogram(signal, bins='auto', density=True)\n",
    "        hist = hist / hist.sum()\n",
    "        data_entropy[chan] = entropy(hist)\n",
    "    \n",
    "    # Log transform of non-negative entropy\n",
    "    data_entropy = np.log10(data_entropy + 1)\n",
    "    \n",
    "    # Create new row\n",
    "    data_to_append = pd.DataFrame({'entropy': data_entropy.flatten()})\n",
    "    \n",
    "    return pd.concat([ieeg_normal, data_to_append], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate dfs for each feature type\n",
    "hup_psd_features = pd.DataFrame()\n",
    "hup_entropy_1min_features = pd.DataFrame()\n",
    "hup_entropy_full_features = pd.DataFrame()\n",
    "\n",
    "# Process HUP data\n",
    "for patient in hup_patient_ids:\n",
    "    patient_el_ids = np.where(hup_idx_map_arr == patient)[0]\n",
    "    \n",
    "    for idx in patient_el_ids:\n",
    "        electrode_data = hup_ts.iloc[:, idx].values\n",
    "        # Get PSD features\n",
    "        hup_psd_features = get_norm_psd(hup_psd_features, electrode_data)\n",
    "        # Get 1-minute entropy features\n",
    "        hup_entropy_1min_features = get_norm_entropy_1min_seg(hup_entropy_1min_features, electrode_data)\n",
    "        # Get full time series entropy features\n",
    "        hup_entropy_fullts_features = get_norm_entropy_full_ts(hup_entropy_full_features, electrode_data)\n",
    "\n",
    "# Combine all HUP features\n",
    "hup_features = pd.concat([\n",
    "    hup_psd_features,\n",
    "    hup_entropy_1min_features[['entropy']].rename(columns={'entropy': 'entropy_1min'}),\n",
    "    hup_entropy_fullts_features[['entropy_mean', 'entropy_std']]\n",
    "], axis=1)\n",
    "\n",
    "# Initialize for MNI data\n",
    "mni_psd_features = pd.DataFrame()\n",
    "mni_entropy_1min_features = pd.DataFrame()\n",
    "mni_entropy_fullts_features = pd.DataFrame()\n",
    "\n",
    "# Process MNI data\n",
    "for patient in mni_patient_ids:\n",
    "    patient_el_ids = np.where(mni_idx_map_arr == patient)[0]\n",
    "    \n",
    "    for idx in patient_el_ids:\n",
    "        electrode_data = mni_ts.iloc[:, idx].values\n",
    "        mni_psd_features = get_norm_psd(mni_psd_features, electrode_data)\n",
    "        mni_entropy_1min_features = get_norm_entropy_1min_seg(mni_entropy_1min_features, electrode_data)\n",
    "        mni_entropy_fullts_features = get_norm_entropy_full_ts(mni_entropy_fullts_features, electrode_data)\n",
    "\n",
    "# Combine all MNI features\n",
    "mni_features = pd.concat([\n",
    "    mni_psd_features,\n",
    "    mni_entropy_1min_features[['entropy']].rename(columns={'entropy': 'entropy_1min'}),\n",
    "    mni_entropy_fullts_features[['entropy_mean', 'entropy_std']]\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "# # Get the ID of the first patient in the HUP dataset\n",
    "# first_patient_id = hup_patient_ids[0]\n",
    "\n",
    "# # Create a boolean mask for electrodes belonging to the first patient\n",
    "# first_patient_mask = (hup_patients[0] == first_patient_id).values.flatten()\n",
    "\n",
    "# # Filter the time-series data to include only electrodes for the first patient\n",
    "# first_patient_electrodes = hup_ts.loc[:, first_patient_mask]\n",
    "\n",
    "# # Count the number of electrodes for the first patient\n",
    "# num_electrodes_first_patient = first_patient_electrodes.shape[1]\n",
    "\n",
    "# print(num_electrodes_first_patient)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnt_gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
