{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import joblib \n",
    "import dill \n",
    "\n",
    "import mne\n",
    "from scipy.signal import welch, get_window\n",
    "from scipy.signal.windows import hamming\n",
    "\n",
    "from scipy.signal import butter, lfilter, sosfilt, filtfilt, sosfreqz, iirnotch\n",
    "from scipy import fftpack\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path_data = '../Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TIME SERIES DATA\n",
    "\n",
    "# dict_keys(['__header__', '__version__', '__globals__', 'SamplingFrequency', 'depth_elecs', 'mni_coords', 'patient_no', 'resected_ch', 'soz_ch', 'spike_24h', 'wake_clip'])\n",
    "hup_atlas = sio.loadmat(os.path.join(base_path_data, 'HUP_atlas.mat'))\n",
    "# dict_keys(['__header__', '__version__', '__globals__', 'AgeAtTimeOfStudy', 'ChannelName', 'ChannelPosition', 'ChannelRegion', 'ChannelType', \n",
    "# 'Data_N2', 'Data_N3', 'Data_R', 'Data_W', 'FacesLeft', 'FacesRight', 'Gender', 'Hemisphere', 'NodesLeft', 'NodesLeftInflated', 'NodesRegionLeft', \n",
    "# 'NodesRegionRight', 'NodesRight', 'NodesRightInflated', 'Patient', 'RegionName', 'SamplingFrequency'])\n",
    "mni_atlas = sio.loadmat(os.path.join(base_path_data, 'MNI_atlas.mat'))\n",
    "\n",
    "# (12000, 3431) @ x-axis is time steps, y-axis is electrodes\n",
    "hup_ts = pd.DataFrame(hup_atlas['wake_clip'])\n",
    "# (13600, 1765)  @ x-axis is time steps, y-axis is electrodes\n",
    "mni_ts = pd.DataFrame(mni_atlas['Data_W'])\n",
    "\n",
    "# accessing columns for patients\n",
    "hup_patients = pd.DataFrame(hup_atlas['patient_no'])\n",
    "mni_patients = pd.DataFrame(mni_atlas['Patient'])\n",
    "\n",
    "# electrode counts\n",
    "hup_patient_total_el_counts = len(hup_atlas['patient_no'])\n",
    "mni_patient_total_el_counts = len(mni_atlas['Patient'])\n",
    "\n",
    "# unique patient ids\n",
    "hup_patient_ids = np.unique(hup_atlas['patient_no'])\n",
    "mni_patient_ids = np.unique(mni_atlas['Patient']) \n",
    "\n",
    "# sampling frequency\n",
    "mni_samp_freq = int(mni_atlas['SamplingFrequency'].flatten()[~np.isnan(mni_atlas['SamplingFrequency'].flatten())][0])\n",
    "hup_samp_freq = int(hup_atlas['SamplingFrequency'].flatten()[~np.isnan(hup_atlas['SamplingFrequency'].flatten())][0])\n",
    "\n",
    "# mapping electrodes to their respective patients\n",
    "hup_patient_numbers = hup_atlas['patient_no'].flatten()\n",
    "hup_el_to_pat_map_dict = {}\n",
    "for idx, patient_num in enumerate(hup_patient_numbers):\n",
    "    hup_el_to_pat_map_dict[idx] = patient_num\n",
    "hup_idx_map_arr = np.array([patient_num for patient_num in hup_patient_numbers]) # arr equivalent\n",
    "\n",
    "mni_patient_numbers = mni_atlas['Patient'].flatten()\n",
    "mni_el_to_pat_map_dict = {}\n",
    "for idx, patient_num in enumerate(mni_patient_numbers):\n",
    "    mni_el_to_pat_map_dict[idx] = patient_num\n",
    "mni_idx_map_arr = np.array([patient_num for patient_num in mni_patient_numbers])\n",
    "\n",
    "## REGION MAPS\n",
    "\n",
    "dk_atlas_df = pd.read_csv(os.path.join(base_path_data, 'desikanKilliany.csv'))\n",
    "# columns: Index(['x', 'y', 'z', 'roiNum', 'snum', 'abvr', 'lobe', 'isSideLeft'], dtype='object')\n",
    "hup_df = pd.read_csv(os.path.join(base_path_data, 'hup_df.csv'))\n",
    "# columns: Index(['x', 'y', 'z', 'roiNum', 'snum', 'abvr', 'lobe', 'isSideLeft'], dtype='object')\n",
    "mni_df = pd.read_csv(os.path.join(base_path_data, 'mni_df.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norm_psd(iEEGnormal, data_timeS, sampling_frequency=200):\n",
    "    \"\"\"\n",
    "    Function to compute normalized power spectral densities for different EEG frequency bands.\n",
    "    \n",
    "    Args:\n",
    "    iEEGnormal (DataFrame): A DataFrame to append results to.\n",
    "    data_timeS (array): Time domain EEG data for a single electrode (1D array)\n",
    "    sampling_frequency (int): Sampling frequency of the EEG data.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: Updated DataFrame with new EEG features.\n",
    "    \"\"\"\n",
    "    \n",
    "    Fs = sampling_frequency\n",
    "    window = Fs * 2\n",
    "    NFFT = window\n",
    "    \n",
    "    # Compute PSD\n",
    "    f, data_psd = welch(data_timeS, fs=Fs, window=hamming(window), \n",
    "                       nfft=NFFT, scaling='density', noverlap=window//2)\n",
    "    \n",
    "    # filter out noise frequency 57.5Hz to 62.5Hz\n",
    "    noise_mask = (f >= 57.5) & (f <= 62.5)\n",
    "    f = f[~noise_mask]\n",
    "    # Handle 1D data_psd\n",
    "    data_psd = data_psd[~noise_mask]\n",
    "    \n",
    "    def bandpower(psd, freqs, freq_range):\n",
    "        \"\"\"Calculate power in the given frequency range.\"\"\"\n",
    "        idx = np.logical_and(freqs >= freq_range[0], freqs <= freq_range[1])\n",
    "        return np.trapezoid(psd[idx], freqs[idx]) \n",
    "    \n",
    "    # Define frequency bands\n",
    "    bands = {'delta': (1, 4), 'theta': (4, 8), 'alpha': (8, 13), \n",
    "             'beta': (13, 30), 'gamma': (30, 80), 'broad': (1, 80)}\n",
    "    \n",
    "    # Calculate band powers (using 1D data_psd)\n",
    "    band_powers = {band: bandpower(data_psd, f, freq_range) \n",
    "                  for band, freq_range in bands.items()}\n",
    "    \n",
    "    # Compute log transform\n",
    "    log_band_powers = {f'{band}log': np.log10(power + 1) \n",
    "                      for band, power in band_powers.items()}\n",
    "    \n",
    "    # Calculate total power\n",
    "    total_band_power = np.sum([value for value in log_band_powers.values()])\n",
    "    \n",
    "    # Calculate relative powers\n",
    "    relative_band_powers = {f'{band}Rel': log_band_powers[f'{band}log'] / total_band_power \n",
    "                          for band in bands}\n",
    "    \n",
    "    # Create DataFrame row\n",
    "    data_to_append = pd.DataFrame([relative_band_powers])\n",
    "    # data_to_append['broadlog'] = log_band_powers['broadlog']\n",
    "    \n",
    "    # Append to existing DataFrame\n",
    "    iEEGnormal = pd.concat([iEEGnormal, data_to_append], ignore_index=True)\n",
    "    \n",
    "    return iEEGnormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not actually used\n",
    "def compute_shannon_entropy(signal):\n",
    "    \"\"\"\n",
    "    Compute Shannon entropy from EEG signal\n",
    "    \n",
    "    Parameters:\n",
    "    signal: 1D array of EEG values\n",
    "    \n",
    "    Returns:\n",
    "    float: Shannon entropy value\n",
    "    \"\"\"\n",
    "    # 1. Estimate probability distribution using histogram\n",
    "    hist, bin_edges = np.histogram(signal, bins='auto', density=True)\n",
    "    \n",
    "    # 2. Normalize to ensure probabilities sum to 1\n",
    "    probabilities = hist / hist.sum()\n",
    "    \n",
    "    # 3. Remove any zero probabilities (since log(0) is undefined)\n",
    "    probabilities = probabilities[probabilities > 0]\n",
    "    \n",
    "    # 4. Compute Shannon entropy\n",
    "    H = -np.sum(probabilities * np.log2(probabilities))\n",
    "    \n",
    "    # 5. Log transform (optional, for feature scaling)\n",
    "    H_log = np.log10(H + 1)\n",
    "    \n",
    "    return H_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norm_entropy_full_ts(ieeg_normal, data_time_s, sampling_frequency=200, window_size_mins=1, stride_mins=0.5):\n",
    "    if data_time_s.ndim == 1:\n",
    "        data_time_s = data_time_s.reshape(-1,1)\n",
    "    \n",
    "    # Calculate window parameters\n",
    "    samples_per_window = sampling_frequency * 60 * window_size_mins\n",
    "    stride_samples = sampling_frequency * 60 * stride_mins\n",
    "    n_windows = int((len(data_time_s) - samples_per_window) // stride_samples + 1)\n",
    "    \n",
    "    entropies = []\n",
    "    for i in range(n_windows):\n",
    "        start_idx = int(i * stride_samples)\n",
    "        end_idx = int(start_idx + samples_per_window)\n",
    "        window_data = data_time_s[start_idx:end_idx, :]\n",
    "        \n",
    "        # Apply filters\n",
    "        b, a = butter(3, 80/(sampling_frequency/2), btype='low')\n",
    "        filtered = filtfilt(b, a, window_data.astype(float), axis=0)\n",
    "        \n",
    "        b, a = butter(3, 1/(sampling_frequency/2), btype='high')\n",
    "        filtered = filtfilt(b, a, filtered, axis=0)\n",
    "        \n",
    "        b, a = iirnotch(60, 30, sampling_frequency)\n",
    "        filtered = filtfilt(b, a, filtered, axis=0)\n",
    "        \n",
    "        # Compute entropy for this window\n",
    "        signal = filtered[:, 0]\n",
    "        \n",
    "        # 1. Estimate probability distribution\n",
    "        hist, _ = np.histogram(signal, bins='auto', density=True)\n",
    "        \n",
    "        # 2. Normalize probabilities\n",
    "        probabilities = hist / hist.sum()\n",
    "        \n",
    "        # 3. Remove zeros\n",
    "        probabilities = probabilities[probabilities > 0]\n",
    "        \n",
    "        # 4. Compute Shannon entropy\n",
    "        H = -np.sum(probabilities * np.log2(probabilities))\n",
    "        \n",
    "        entropies.append(H)\n",
    "    \n",
    "    # Compute statistics across windows\n",
    "    mean_entropy = np.mean(entropies)\n",
    "    std_entropy = np.std(entropies)\n",
    "    \n",
    "    # 5. Log transform of mean entropy\n",
    "    mean_entropy_log = np.log10(mean_entropy + 1)\n",
    "    \n",
    "    # Create features\n",
    "    data_to_append = pd.DataFrame({\n",
    "        'entropy_mean': [mean_entropy_log],\n",
    "        'entropy_std': [std_entropy]\n",
    "    })\n",
    "    \n",
    "    return pd.concat([ieeg_normal, data_to_append], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norm_entropy_1min_seg(ieeg_normal, data_time_s, sampling_frequency=200):\n",
    "    if data_time_s.ndim == 1:\n",
    "        data_time_s = data_time_s.reshape(-1,1)\n",
    "    \n",
    "    # Get first minute of data\n",
    "    data_seg = data_time_s[:sampling_frequency*60, :]\n",
    "    \n",
    "    # Low pass filter at 80Hz\n",
    "    b, a = butter(3, 80/(sampling_frequency/2), btype='low')\n",
    "    data_seg_filtered = filtfilt(b, a, data_seg.astype(float), axis=0)\n",
    "    \n",
    "    # High pass filter at 1Hz  \n",
    "    b, a = butter(3, 1/(sampling_frequency/2), btype='high')\n",
    "    data_seg_filtered = filtfilt(b, a, data_seg_filtered, axis=0)\n",
    "    \n",
    "    # Notch filter at 60Hz\n",
    "    b, a = iirnotch(60, 30, sampling_frequency)\n",
    "    data_seg_notch = filtfilt(b, a, data_seg_filtered, axis=0)\n",
    "\n",
    "    # Compute Shannon entropy for each channel\n",
    "    data_entropy = np.zeros((data_seg_notch.shape[1], 1))\n",
    "    for chan in range(data_seg_notch.shape[1]):\n",
    "        signal = data_seg_notch[:, chan]\n",
    "        # Estimate probability distribution\n",
    "        hist, _ = np.histogram(signal, bins='auto', density=True)\n",
    "        hist = hist / hist.sum()\n",
    "        data_entropy[chan] = entropy(hist)\n",
    "    \n",
    "    # Log transform of non-negative entropy\n",
    "    data_entropy = np.log10(data_entropy + 1)\n",
    "    \n",
    "    # Create new row\n",
    "    data_to_append = pd.DataFrame({'entropy': data_entropy.flatten()})\n",
    "    \n",
    "    return pd.concat([ieeg_normal, data_to_append], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DataFrames - consistent naming with \"fullts\"\n",
    "hup_psd_features = pd.DataFrame()\n",
    "hup_entropy_1min_features = pd.DataFrame()\n",
    "hup_entropy_fullts_features = pd.DataFrame()  # Changed from \"full_features\"\n",
    "\n",
    "# Process HUP data\n",
    "for patient in hup_patient_ids:\n",
    "    patient_el_ids = np.where(hup_idx_map_arr == patient)[0]\n",
    "    \n",
    "    for idx in patient_el_ids:\n",
    "        electrode_data = hup_ts.iloc[:, idx].values\n",
    "        hup_psd_features = get_norm_psd(hup_psd_features, electrode_data)\n",
    "        hup_entropy_1min_features = get_norm_entropy_1min_seg(hup_entropy_1min_features, electrode_data)\n",
    "        hup_entropy_fullts_features = get_norm_entropy_full_ts(hup_entropy_fullts_features, electrode_data)  # Now matches variable name\n",
    "\n",
    "# Combine HUP features - using consistent name\n",
    "hup_features = pd.concat([\n",
    "    hup_psd_features,\n",
    "    hup_entropy_1min_features[['entropy']].rename(columns={'entropy': 'entropy_1min'}),\n",
    "    hup_entropy_fullts_features[['entropy_mean', 'entropy_std']]  # Now matches variable name\n",
    "], axis=1)\n",
    "\n",
    "# MNI DataFrames \n",
    "mni_psd_features = pd.DataFrame()\n",
    "mni_entropy_1min_features = pd.DataFrame()\n",
    "mni_entropy_fullts_features = pd.DataFrame()\n",
    "\n",
    "# Process MNI data\n",
    "for patient in mni_patient_ids:\n",
    "    patient_el_ids = np.where(mni_idx_map_arr == patient)[0]\n",
    "    \n",
    "    for idx in patient_el_ids:\n",
    "        electrode_data = mni_ts.iloc[:, idx].values\n",
    "        mni_psd_features = get_norm_psd(mni_psd_features, electrode_data)\n",
    "        mni_entropy_1min_features = get_norm_entropy_1min_seg(mni_entropy_1min_features, electrode_data)\n",
    "        mni_entropy_fullts_features = get_norm_entropy_full_ts(mni_entropy_fullts_features, electrode_data)\n",
    "\n",
    "# Combine MNI features\n",
    "mni_features = pd.concat([\n",
    "    mni_psd_features,\n",
    "    mni_entropy_1min_features[['entropy']].rename(columns={'entropy': 'entropy_1min'}),\n",
    "    mni_entropy_fullts_features[['entropy_mean', 'entropy_std']]\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUP Features:\n",
      "Shape: (3431, 9)\n",
      "Columns: ['deltaRel', 'thetaRel', 'alphaRel', 'betaRel', 'gammaRel', 'broadRel', 'entropy_1min', 'entropy_mean', 'entropy_std']\n",
      "\n",
      "First few rows:\n",
      "   deltaRel  thetaRel  alphaRel   betaRel  gammaRel  broadRel  entropy_1min  \\\n",
      "0  0.177924  0.147311  0.135588  0.173211  0.139114  0.226852      0.660497   \n",
      "1  0.210691  0.231001  0.098005  0.129331  0.048090  0.282882      0.664082   \n",
      "2  0.248122  0.184206  0.116251  0.126453  0.059346  0.265622      0.662271   \n",
      "3  0.214907  0.162279  0.114287  0.135578  0.073299  0.299651      0.659488   \n",
      "4  0.187794  0.182233  0.136686  0.152295  0.069230  0.271761      0.657976   \n",
      "\n",
      "   entropy_mean  entropy_std  \n",
      "0      0.789527          0.0  \n",
      "1      0.793369          0.0  \n",
      "2      0.791429          0.0  \n",
      "3      0.788446          0.0  \n",
      "4      0.786825          0.0  \n",
      "\n",
      "Basic statistics:\n",
      "           deltaRel     thetaRel     alphaRel      betaRel     gammaRel  \\\n",
      "count  3431.000000  3431.000000  3431.000000  3431.000000  3431.000000   \n",
      "mean      0.195423     0.172769     0.153454     0.148689     0.098853   \n",
      "std       0.031672     0.018485     0.022784     0.024257     0.029283   \n",
      "min       0.079483     0.067368     0.030700     0.035900     0.014172   \n",
      "25%       0.173443     0.162974     0.141779     0.135374     0.080027   \n",
      "50%       0.192697     0.172821     0.152823     0.151037     0.102781   \n",
      "75%       0.212900     0.182852     0.164543     0.163997     0.119706   \n",
      "max       0.376323     0.277389     0.294002     0.292550     0.177618   \n",
      "\n",
      "          broadRel  entropy_1min  entropy_mean  entropy_std  \n",
      "count  3431.000000   3431.000000   3431.000000       3431.0  \n",
      "mean      0.230813      0.667202      0.796707          0.0  \n",
      "std       0.027647      0.006298      0.006737          0.0  \n",
      "min       0.196556      0.651728      0.780122          0.0  \n",
      "25%       0.213350      0.662786      0.791980          0.0  \n",
      "50%       0.222750      0.665913      0.795330          0.0  \n",
      "75%       0.239422      0.670302      0.800029          0.0  \n",
      "max       0.464221      0.748925      0.883668          0.0   \n",
      "\n",
      "\n",
      "MNI Features:\n",
      "Shape: (1765, 9)\n",
      "Columns: ['deltaRel', 'thetaRel', 'alphaRel', 'betaRel', 'gammaRel', 'broadRel', 'entropy_1min', 'entropy_mean', 'entropy_std']\n",
      "\n",
      "First few rows:\n",
      "   deltaRel  thetaRel  alphaRel   betaRel  gammaRel  broadRel  entropy_1min  \\\n",
      "0  0.157038  0.143701  0.155727  0.200617  0.096841  0.246077      0.665212   \n",
      "1  0.166424  0.141783  0.145537  0.198588  0.105052  0.242617      0.665628   \n",
      "2  0.162231  0.146422  0.150696  0.195236  0.111818  0.233596      0.667045   \n",
      "3  0.169508  0.139919  0.160346  0.205149  0.092476  0.232602      0.672854   \n",
      "4  0.171363  0.157544  0.150542  0.159429  0.133543  0.227579      0.660791   \n",
      "\n",
      "   entropy_mean  entropy_std  \n",
      "0      0.794580          0.0  \n",
      "1      0.795025          0.0  \n",
      "2      0.796542          0.0  \n",
      "3      0.802759          0.0  \n",
      "4      0.789843          0.0  \n",
      "\n",
      "Basic statistics:\n",
      "           deltaRel     thetaRel     alphaRel      betaRel     gammaRel  \\\n",
      "count  1765.000000  1765.000000  1765.000000  1765.000000  1765.000000   \n",
      "mean      0.178371     0.168981     0.157086     0.163744     0.100066   \n",
      "std       0.024600     0.020741     0.022248     0.022897     0.029884   \n",
      "min       0.085966     0.087047     0.074283     0.069782     0.013149   \n",
      "25%       0.162217     0.155562     0.142463     0.150639     0.080891   \n",
      "50%       0.177869     0.166918     0.155314     0.162416     0.103560   \n",
      "75%       0.192630     0.179845     0.171038     0.176666     0.121759   \n",
      "max       0.296321     0.269815     0.251089     0.289101     0.194566   \n",
      "\n",
      "          broadRel  entropy_1min  entropy_mean  entropy_std  \n",
      "count  1765.000000   1765.000000   1765.000000       1765.0  \n",
      "mean      0.231752      0.665009      0.794360          0.0  \n",
      "std       0.020353      0.004354      0.004662          0.0  \n",
      "min       0.202262      0.652029      0.780445          0.0  \n",
      "25%       0.217895      0.661918      0.791051          0.0  \n",
      "50%       0.227018      0.664129      0.793419          0.0  \n",
      "75%       0.240215      0.667155      0.796660          0.0  \n",
      "max       0.334327      0.685827      0.816622          0.0   \n",
      "\n",
      "Missing values in HUP: deltaRel        0\n",
      "thetaRel        0\n",
      "alphaRel        0\n",
      "betaRel         0\n",
      "gammaRel        0\n",
      "broadRel        0\n",
      "entropy_1min    0\n",
      "entropy_mean    0\n",
      "entropy_std     0\n",
      "dtype: int64\n",
      "Missing values in MNI: deltaRel        0\n",
      "thetaRel        0\n",
      "alphaRel        0\n",
      "betaRel         0\n",
      "gammaRel        0\n",
      "broadRel        0\n",
      "entropy_1min    0\n",
      "entropy_mean    0\n",
      "entropy_std     0\n",
      "dtype: int64\n",
      "\n",
      "HUP Feature Ranges:\n",
      "deltaRel: [0.079, 0.376]\n",
      "thetaRel: [0.067, 0.277]\n",
      "alphaRel: [0.031, 0.294]\n",
      "betaRel: [0.036, 0.293]\n",
      "gammaRel: [0.014, 0.178]\n",
      "broadRel: [0.197, 0.464]\n",
      "entropy_1min: [0.652, 0.749]\n",
      "entropy_mean: [0.780, 0.884]\n",
      "entropy_std: [0.000, 0.000]\n",
      "\n",
      "MNI Feature Ranges:\n",
      "deltaRel: [0.086, 0.296]\n",
      "thetaRel: [0.087, 0.270]\n",
      "alphaRel: [0.074, 0.251]\n",
      "betaRel: [0.070, 0.289]\n",
      "gammaRel: [0.013, 0.195]\n",
      "broadRel: [0.202, 0.334]\n",
      "entropy_1min: [0.652, 0.686]\n",
      "entropy_mean: [0.780, 0.817]\n",
      "entropy_std: [0.000, 0.000]\n"
     ]
    }
   ],
   "source": [
    "# Basic shape and info\n",
    "print(\"HUP Features:\")\n",
    "print(f\"Shape: {hup_features.shape}\")\n",
    "print(f\"Columns: {hup_features.columns.tolist()}\\n\")\n",
    "print(f\"First few rows:\\n{hup_features.head()}\\n\")\n",
    "print(\"Basic statistics:\\n\", hup_features.describe(), \"\\n\")\n",
    "\n",
    "print(\"\\nMNI Features:\")\n",
    "print(f\"Shape: {mni_features.shape}\")\n",
    "print(f\"Columns: {mni_features.columns.tolist()}\\n\")\n",
    "print(f\"First few rows:\\n{mni_features.head()}\\n\")\n",
    "print(\"Basic statistics:\\n\", mni_features.describe(), \"\\n\")\n",
    "\n",
    "# Check for any missing values\n",
    "print(\"Missing values in HUP:\", hup_features.isnull().sum())\n",
    "print(\"Missing values in MNI:\", mni_features.isnull().sum())\n",
    "\n",
    "# Value ranges for each feature\n",
    "print(\"\\nHUP Feature Ranges:\")\n",
    "for col in hup_features.columns:\n",
    "    print(f\"{col}: [{hup_features[col].min():.3f}, {hup_features[col].max():.3f}]\")\n",
    "\n",
    "print(\"\\nMNI Feature Ranges:\")\n",
    "for col in mni_features.columns:\n",
    "    print(f\"{col}: [{mni_features[col].min():.3f}, {mni_features[col].max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = 'results'\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "\n",
    "# combined features\n",
    "hup_features.to_csv(os.path.join(results_dir, 'hup_univar_feats.csv'), index=False)\n",
    "mni_features.to_csv(os.path.join(results_dir, 'mni_univar_feats.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggreggate features based on ROI mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_features_by_region(features_df, region_df, dk_atlas_df):\n",
    "    \"\"\"\n",
    "    Aggregate electrode-level features to region-level features.\n",
    "    \n",
    "    Args:\n",
    "        features_df: DataFrame with electrode features\n",
    "        region_df: DataFrame with electrode to region mapping (has roiNum)\n",
    "        dk_atlas_df: DataFrame with region names and numbers (has roiNum and roi)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with region-level features including both mean and median\n",
    "    \"\"\"\n",
    "    # Combine features with region numbers\n",
    "    combined_df = pd.concat([\n",
    "        features_df.reset_index(drop=True),\n",
    "        region_df[['roiNum']].reset_index(drop=True)\n",
    "    ], axis=1)\n",
    "    \n",
    "    # Calculate mean and median for each feature by region\n",
    "    agg_dict = {col: ['mean', 'median'] for col in features_df.columns}\n",
    "    \n",
    "    # Aggregate by roiNum\n",
    "    region_features = combined_df.groupby('roiNum').agg(agg_dict)\n",
    "    \n",
    "    # Flatten column names\n",
    "    region_features.columns = [f'{col[0]}_{col[1]}' for col in region_features.columns]\n",
    "    \n",
    "    # Add region names from DK atlas\n",
    "    region_features = region_features.join(\n",
    "        dk_atlas_df[['roiNum', 'roi']].drop_duplicates('roiNum').set_index('roiNum')\n",
    "    )\n",
    "    \n",
    "    return region_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "hup_univar_feats = hup_features\n",
    "mni_univar_feats = mni_features\n",
    "\n",
    "hup_region_features = aggregate_features_by_region(hup_univar_feats, hup_df, dk_atlas_df)\n",
    "mni_region_features = aggregate_features_by_region(mni_univar_feats, mni_df, dk_atlas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "hup_region_features.to_csv(os.path.join(results_dir, 'hup_univar_feats_reg.csv'), index=False)\n",
    "mni_region_features.to_csv(os.path.join(results_dir, 'mni_univar_feats_reg.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnt_gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
