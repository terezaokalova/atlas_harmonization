{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global configurations\n",
    "code_directory = '/Users/tereza/nishant/atlas/atlas_work_terez/atlas_harmonization/code'\n",
    "os.chdir(code_directory)\n",
    "\n",
    "# RESULTS_DIR = '../results'\n",
    "# DATA_DIR = '../Data'\n",
    "# FIGURES_DIR = '../figures'\n",
    "\n",
    "# if not os.path.exists(FIGURES_DIR):\n",
    "#     os.makedirs(FIGURES_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR = '../results'\n",
    "DATA_DIR = '../Data'\n",
    "FIGURES_DIR = '../figures'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Feature definitions\n",
    "# FEATURE_COLUMNS = [\n",
    "#     'deltaRel_mean', \n",
    "#     'thetaRel_mean', \n",
    "#     'alphaRel_mean', \n",
    "#     'betaRel_mean',\n",
    "#     'gammaRel_mean',\n",
    "#     'entropy_1min_mean',\n",
    "#     'entropy_fullts_mean'\n",
    "# ]\n",
    "\n",
    "# FEATURE_NAMES = {\n",
    "#     'deltaRel_mean': 'Delta Band (0.5-4 Hz)',\n",
    "#     'thetaRel_mean': 'Theta Band (4-8 Hz)',\n",
    "#     'alphaRel_mean': 'Alpha Band (8-13 Hz)',\n",
    "#     'betaRel_mean': 'Beta Band (13-30 Hz)',\n",
    "#     'gammaRel_mean': 'Gamma Band (30-80 Hz)',\n",
    "#     'entropy_1min_mean': 'Signal Entropy (1-min)',\n",
    "#     'entropy_fullts_mean': 'Signal Entropy (full)'\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegionalAnalysis:\n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.feature_columns = [\n",
    "            'deltaRel_mean', \n",
    "            'thetaRel_mean', \n",
    "            'alphaRel_mean', \n",
    "            'betaRel_mean',\n",
    "            'gammaRel_mean',\n",
    "            'entropy_1min_mean',\n",
    "            'entropy_fullts_mean'\n",
    "        ]\n",
    "        \n",
    "    def prepare_data(self, region: str, feature: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Prepare data for unpaired statistical comparison\n",
    "        \"\"\"\n",
    "        # Get data for specific region\n",
    "        hup_region_data = self.hup_features[self.hup_features['roi'] == region]\n",
    "        mni_region_data = self.mni_features[self.mni_features['roi'] == region]\n",
    "        \n",
    "        # Group by patient and get mean values\n",
    "        hup_data = hup_region_data.groupby('patient_id')[feature].mean().values\n",
    "        mni_data = mni_region_data.groupby('patient_id')[feature].mean().values\n",
    "        \n",
    "        # Check minimum sample sizes\n",
    "        if len(hup_data) < 5 or len(mni_data) < 5:\n",
    "            raise ValueError(f\"Insufficient samples for region {region} (HUP: {len(hup_data)}, MNI: {len(mni_data)})\")\n",
    "            \n",
    "        return hup_data, mni_data\n",
    "    \n",
    "    def compute_effect_size(self, hup_data: np.ndarray, mni_data: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Compute Cohen's d effect size for unpaired data\n",
    "        \"\"\"\n",
    "        n1, n2 = len(hup_data), len(mni_data)\n",
    "        var1, var2 = np.var(hup_data, ddof=1), np.var(mni_data, ddof=1)\n",
    "        \n",
    "        # Pooled standard deviation\n",
    "        pooled_se = np.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2))\n",
    "        \n",
    "        # Effect size (Cohen's d)\n",
    "        d = (np.mean(hup_data) - np.mean(mni_data)) / pooled_se\n",
    "        return d\n",
    "    \n",
    "    def analyze_regions(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Perform unpaired regional analysis between HUP and MNI cohorts\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for region in self.common_regions:\n",
    "            self.logger.info(f\"Analyzing region: {region}\")\n",
    "            \n",
    "            for feature in self.feature_columns:\n",
    "                try:\n",
    "                    # Get data\n",
    "                    hup_data, mni_data = self.prepare_data(region, feature)\n",
    "                    \n",
    "                    # Perform Mann-Whitney U test (unpaired)\n",
    "                    statistic, pvalue = stats.mannwhitneyu(\n",
    "                        hup_data, \n",
    "                        mni_data,\n",
    "                        alternative='two-sided'\n",
    "                    )\n",
    "                    \n",
    "                    # Compute effect size\n",
    "                    effect_size = self.compute_effect_size(hup_data, mni_data)\n",
    "                    \n",
    "                    results.append({\n",
    "                        'region': region,\n",
    "                        'feature': feature,\n",
    "                        'statistic': statistic,\n",
    "                        'pvalue': pvalue,\n",
    "                        'effect_size': effect_size,\n",
    "                        'hup_mean': np.mean(hup_data),\n",
    "                        'mni_mean': np.mean(mni_data),\n",
    "                        'hup_std': np.std(hup_data),\n",
    "                        'mni_std': np.std(mni_data),\n",
    "                        'hup_n': len(hup_data),\n",
    "                        'mni_n': len(mni_data)\n",
    "                    })\n",
    "                    \n",
    "                except ValueError as e:\n",
    "                    self.logger.warning(f\"Skipping {region}-{feature}: {str(e)}\")\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Error processing {region}-{feature}: {str(e)}\")\n",
    "        \n",
    "        # Create results DataFrame\n",
    "        results_df = pd.DataFrame(results)\n",
    "        \n",
    "        if len(results_df) > 0:\n",
    "            # Apply FDR correction across all tests\n",
    "            _, fdr_pvals = fdrcorrection(results_df['pvalue'])\n",
    "            results_df['pvalue_fdr'] = fdr_pvals\n",
    "        \n",
    "        return results_df\n",
    "\n",
    "    def summarize_results(self, results_df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Print summary of findings\n",
    "        \"\"\"\n",
    "        if len(results_df) == 0:\n",
    "            print(\"No results to summarize\")\n",
    "            return\n",
    "            \n",
    "        print(\"\\nRegional Analysis Summary\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"\\nTotal tests performed: {len(results_df)}\")\n",
    "        print(f\"Number of regions tested: {results_df['region'].nunique()}\")\n",
    "        print(f\"Number of features tested: {results_df['feature'].nunique()}\")\n",
    "        \n",
    "        sig_results = results_df[results_df['pvalue_fdr'] < 0.05]\n",
    "        print(f\"\\nSignificant results after FDR correction: {len(sig_results)}\")\n",
    "        \n",
    "        if len(sig_results) > 0:\n",
    "            print(\"\\nSignificant differences found:\")\n",
    "            for feature in self.feature_columns:\n",
    "                feature_results = sig_results[sig_results['feature'] == feature]\n",
    "                if len(feature_results) > 0:\n",
    "                    print(f\"\\n{feature}:\")\n",
    "                    for _, row in feature_results.iterrows():\n",
    "                        direction = \"higher in HUP\" if row['effect_size'] > 0 else \"higher in MNI\"\n",
    "                        print(f\"  * {row['region']}: {direction}\")\n",
    "                        print(f\"    Effect size (d): {row['effect_size']:.3f}\")\n",
    "                        print(f\"    p-value (FDR): {row['pvalue_fdr']:.3e}\")\n",
    "                        print(f\"    Samples: HUP={row['hup_n']}, MNI={row['mni_n']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RegionalAnalysis:\n",
    "#     def __init__(self):\n",
    "#         self.logger = logging.getLogger(__name__)\n",
    "#         self.feature_columns = [\n",
    "#             'deltaRel_mean', \n",
    "#             'thetaRel_mean', \n",
    "#             'alphaRel_mean', \n",
    "#             'betaRel_mean',\n",
    "#             'gammaRel_mean',\n",
    "#             'entropy_1min_mean',\n",
    "#             'entropy_fullts_mean'\n",
    "#         ]\n",
    "        \n",
    "#         self.feature_names = {\n",
    "#             'deltaRel_mean': 'Delta Band (0.5-4 Hz)',\n",
    "#             'thetaRel_mean': 'Theta Band (4-8 Hz)',\n",
    "#             'alphaRel_mean': 'Alpha Band (8-13 Hz)',\n",
    "#             'betaRel_mean': 'Beta Band (13-30 Hz)',\n",
    "#             'gammaRel_mean': 'Gamma Band (30-80 Hz)',\n",
    "#             'entropy_1min_mean': 'Signal Entropy (1-min)',\n",
    "#             'entropy_fullts_mean': 'Signal Entropy (full)'\n",
    "#         }\n",
    "        \n",
    "#     def load_data(self) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "#         \"\"\"\n",
    "#         Load and prepare the regional feature data for both cohorts\n",
    "#         \"\"\"\n",
    "#         # Load data\n",
    "#         self.hup_features = pd.read_csv(\"ge_go_hup_region_features.csv\")\n",
    "#         self.mni_features = pd.read_csv(\"mni_region_features.csv\")\n",
    "        \n",
    "#         # Get common regions\n",
    "#         self.common_regions = set(self.hup_features['roi'].unique()) & set(self.mni_features['roi'].unique())\n",
    "        \n",
    "#         print(f\"Loaded data:\")\n",
    "#         print(f\"HUP features shape: {self.hup_features.shape}\")\n",
    "#         print(f\"MNI features shape: {self.mni_features.shape}\")\n",
    "#         print(f\"Number of common regions: {len(self.common_regions)}\")\n",
    "        \n",
    "#         return self.hup_features, self.mni_features\n",
    "    \n",
    "#     def prepare_paired_data(self, region: str, feature: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "#         \"\"\"\n",
    "#         Prepare paired data for a specific region and feature.\n",
    "#         Returns arrays of equal length for valid statistical comparison.\n",
    "#         \"\"\"\n",
    "#         # Get data for specific region\n",
    "#         hup_data = self.hup_features[self.hup_features['roi'] == region][feature].values\n",
    "#         mni_data = self.mni_features[self.mni_features['roi'] == region][feature].values\n",
    "        \n",
    "#         # Get minimum length to ensure paired data\n",
    "#         min_length = min(len(hup_data), len(mni_data))\n",
    "        \n",
    "#         if min_length < 5:  # Minimum sample size requirement\n",
    "#             raise ValueError(f\"Insufficient samples for region {region}\")\n",
    "            \n",
    "#         # Match lengths for pairing\n",
    "#         hup_data = hup_data[:min_length]\n",
    "#         mni_data = mni_data[:min_length]\n",
    "        \n",
    "#         return hup_data, mni_data\n",
    "    \n",
    "#     def compute_effect_size(self, hup_data: np.ndarray, mni_data: np.ndarray) -> float:\n",
    "#         \"\"\"\n",
    "#         Compute Cohen's d effect size for paired data\n",
    "#         \"\"\"\n",
    "#         diff = hup_data - mni_data\n",
    "#         d = np.mean(diff) / np.std(diff)\n",
    "#         return d\n",
    "    \n",
    "#     def analyze_regions(self) -> pd.DataFrame:\n",
    "#         \"\"\"\n",
    "#         Perform paired regional analysis between HUP and MNI cohorts\n",
    "#         \"\"\"\n",
    "#         results = []\n",
    "        \n",
    "#         for region in self.common_regions:\n",
    "#             self.logger.info(f\"Analyzing region: {region}\")\n",
    "            \n",
    "#             for feature in self.feature_columns:\n",
    "#                 try:\n",
    "#                     # Get paired data\n",
    "#                     hup_data, mni_data = self.prepare_paired_data(region, feature)\n",
    "                    \n",
    "#                     # Perform Wilcoxon signed-rank test (paired test)\n",
    "#                     statistic, pvalue = stats.wilcoxon(hup_data, mni_data)\n",
    "                    \n",
    "#                     # Compute effect size\n",
    "#                     effect_size = self.compute_effect_size(hup_data, mni_data)\n",
    "                    \n",
    "#                     results.append({\n",
    "#                         'region': region,\n",
    "#                         'feature': feature,\n",
    "#                         'statistic': statistic,\n",
    "#                         'pvalue': pvalue,\n",
    "#                         'effect_size': effect_size,\n",
    "#                         'hup_mean': np.mean(hup_data),\n",
    "#                         'mni_mean': np.mean(mni_data),\n",
    "#                         'hup_std': np.std(hup_data),\n",
    "#                         'mni_std': np.std(mni_data),\n",
    "#                         'n_samples': len(hup_data)\n",
    "#                     })\n",
    "                    \n",
    "#                 except ValueError as e:\n",
    "#                     self.logger.warning(f\"Skipping {region}-{feature}: {str(e)}\")\n",
    "#                 except Exception as e:\n",
    "#                     self.logger.error(f\"Error processing {region}-{feature}: {str(e)}\")\n",
    "        \n",
    "#         # Create results DataFrame\n",
    "#         results_df = pd.DataFrame(results)\n",
    "        \n",
    "#         # Apply FDR correction\n",
    "#         _, fdr_pvals = fdrcorrection(results_df['pvalue'])\n",
    "#         results_df['pvalue_fdr'] = fdr_pvals\n",
    "        \n",
    "#         return results_df\n",
    "    \n",
    "#     def summarize_results(self, results_df: pd.DataFrame):\n",
    "#         \"\"\"\n",
    "#         Print summary of significant findings\n",
    "#         \"\"\"\n",
    "#         print(\"\\nRegional Analysis Summary\")\n",
    "#         print(\"=\" * 50)\n",
    "        \n",
    "#         for feature in self.feature_columns:\n",
    "#             feature_results = results_df[results_df['feature'] == feature]\n",
    "#             sig_results = feature_results[feature_results['pvalue_fdr'] < 0.05]\n",
    "            \n",
    "#             print(f\"\\n{self.feature_names[feature]}:\")\n",
    "#             print(f\"- {len(sig_results)} regions show significant differences\")\n",
    "            \n",
    "#             if len(sig_results) > 0:\n",
    "#                 sig_results = sig_results.sort_values('effect_size', key=abs, ascending=False)\n",
    "#                 print(\"\\nTop regions with largest differences:\")\n",
    "                \n",
    "#                 for _, row in sig_results.head(3).iterrows():\n",
    "#                     direction = \"higher in HUP\" if row['effect_size'] > 0 else \"higher in MNI\"\n",
    "#                     effect_mag = \"large\" if abs(row['effect_size']) > 0.8 else \\\n",
    "#                                 \"medium\" if abs(row['effect_size']) > 0.5 else \"small\"\n",
    "                    \n",
    "#                     print(f\"  * {row['region']}: {effect_mag} effect {direction}\")\n",
    "#                     print(f\"    (p={row['pvalue_fdr']:.3e}, d={row['effect_size']:.2f}, n={row['n_samples']})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in main execution: 'RegionalAnalysis' object has no attribute 'load_data'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'RegionalAnalysis' object has no attribute 'load_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 28\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[52], line 10\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m analysis \u001b[38;5;241m=\u001b[39m RegionalAnalysis()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43manalysis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Perform analysis\u001b[39;00m\n\u001b[1;32m     13\u001b[0m results \u001b[38;5;241m=\u001b[39m analysis\u001b[38;5;241m.\u001b[39manalyze_regions()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RegionalAnalysis' object has no attribute 'load_data'"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Set up logging\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "    try:\n",
    "        # Initialize analysis\n",
    "        analysis = RegionalAnalysis()\n",
    "        \n",
    "        # Load data\n",
    "        analysis.load_data()\n",
    "        \n",
    "        # Perform analysis\n",
    "        results = analysis.analyze_regions()\n",
    "        \n",
    "        # Summarize results\n",
    "        analysis.summarize_results(results)\n",
    "        \n",
    "        # Save results\n",
    "        output_path = os.path.join(RESULTS_DIR, 'regional_analysis_results.csv')\n",
    "        # results.to_csv(output_path, index=False)\n",
    "        print(f\"\\nResults saved to: {output_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HUP Dataset Analysis:\n",
      "==================================================\n",
      "\n",
      "Total unique regions in HUP: 67\n",
      "Total unique patients in HUP: 28\n",
      "\n",
      "Patient counts per region (top 10):\n",
      "                            roi  n_patients  n_recordings\n",
      "61  ctx-rh-rostralmiddlefrontal          16            16\n",
      "23        ctx-lh-middletemporal          15            15\n",
      "17      ctx-lh-inferiortemporal          15            15\n",
      "38      ctx-lh-superiortemporal          14            14\n",
      "44              ctx-rh-fusiform          14            14\n",
      "46      ctx-rh-inferiortemporal          13            13\n",
      "52        ctx-rh-middletemporal          13            13\n",
      "15              ctx-lh-fusiform          13            13\n",
      "35  ctx-lh-rostralmiddlefrontal          13            13\n",
      "58            ctx-rh-precentral          12            12\n",
      "\n",
      "Regions with < 5 patients:\n",
      "                                roi  n_patients\n",
      "18                    ctx-lh-insula           4\n",
      "36           ctx-lh-superiorfrontal           4\n",
      "0                     Left-Amygdala           4\n",
      "47                    ctx-rh-insula           3\n",
      "50                   ctx-rh-lingual           3\n",
      "55             ctx-rh-parsorbitalis           3\n",
      "33                 ctx-lh-precuneus           3\n",
      "21                   ctx-lh-lingual           3\n",
      "19          ctx-lh-lateraloccipital           3\n",
      "34  ctx-lh-rostralanteriorcingulate           3\n",
      "10                  ctx-lh-bankssts           3\n",
      "4              Right-Accumbens-area           3\n",
      "20      ctx-lh-lateralorbitofrontal           3\n",
      "6                     Right-Caudate           2\n",
      "9                     Right-Putamen           2\n",
      "12                    ctx-lh-cuneus           2\n",
      "37          ctx-lh-superiorparietal           2\n",
      "14               ctx-lh-frontalpole           2\n",
      "13                ctx-lh-entorhinal           2\n",
      "5                    Right-Amygdala           2\n",
      "25           ctx-lh-parahippocampal           2\n",
      "3                      Left-Putamen           2\n",
      "43                ctx-rh-entorhinal           2\n",
      "59                 ctx-rh-precuneus           2\n",
      "60  ctx-rh-rostralanteriorcingulate           1\n",
      "2                     Left-Pallidum           1\n",
      "40              ctx-lh-temporalpole           1\n",
      "8                    Right-Pallidum           1\n",
      "24               ctx-lh-paracentral           1\n",
      "27             ctx-lh-parsorbitalis           1\n",
      "29             ctx-lh-pericalcarine           1\n",
      "31        ctx-lh-posteriorcingulate           1\n",
      "66              ctx-rh-temporalpole           1\n",
      "\n",
      "MNI Dataset Analysis:\n",
      "==================================================\n",
      "\n",
      "Total unique regions in MNI: 68\n",
      "Total unique patients in MNI: 106\n",
      "\n",
      "Patient counts per region (top 10):\n",
      "                            roi  n_patients  n_recordings\n",
      "59            ctx-rh-precentral          25            25\n",
      "62  ctx-rh-rostralmiddlefrontal          22            22\n",
      "50        ctx-rh-middletemporal          21            21\n",
      "34      ctx-lh-superiortemporal          20            20\n",
      "44                ctx-rh-insula          18            18\n",
      "42      ctx-rh-inferiorparietal          17            17\n",
      "63       ctx-rh-superiorfrontal          16            16\n",
      "53       ctx-rh-parsopercularis          16            16\n",
      "32       ctx-lh-superiorfrontal          16            16\n",
      "66         ctx-rh-supramarginal          16            16\n",
      "\n",
      "Regions with < 5 patients:\n",
      "                                roi  n_patients\n",
      "61  ctx-rh-rostralanteriorcingulate           4\n",
      "40                    ctx-rh-cuneus           4\n",
      "23             ctx-lh-parsorbitalis           3\n",
      "25             ctx-lh-pericalcarine           3\n",
      "56             ctx-rh-pericalcarine           3\n",
      "27        ctx-lh-posteriorcingulate           3\n",
      "45          ctx-rh-isthmuscingulate           3\n",
      "3                      Left-Putamen           3\n",
      "10                    ctx-lh-cuneus           3\n",
      "52           ctx-rh-parahippocampal           2\n",
      "4                    Right-Amygdala           2\n",
      "1                      Left-Caudate           2\n",
      "0                     Left-Amygdala           1\n",
      "6                     Right-Putamen           1\n",
      "67        ctx-rh-transversetemporal           1\n",
      "\n",
      "Overlap Analysis:\n",
      "==================================================\n",
      "Number of common regions: 57\n",
      "\n",
      "Regions with ≥5 patients in both cohorts: 33\n",
      "\n",
      "Valid regions for paired analysis:\n",
      "                         region  hup_patients  mni_patients\n",
      "9   ctx-rh-rostralmiddlefrontal            16            22\n",
      "32      ctx-lh-inferiortemporal            15            13\n",
      "2         ctx-lh-middletemporal            15            12\n",
      "28      ctx-lh-superiortemporal            14            20\n",
      "21              ctx-rh-fusiform            14            11\n",
      "8   ctx-lh-rostralmiddlefrontal            13            14\n",
      "29        ctx-rh-middletemporal            13            21\n",
      "18      ctx-rh-inferiortemporal            13            11\n",
      "16              ctx-lh-fusiform            13             6\n",
      "5             ctx-rh-precentral            12            25\n",
      "22      ctx-lh-parstriangularis            11            11\n",
      "3            ctx-rh-postcentral            10            15\n",
      "12  ctx-rh-lateralorbitofrontal            10            10\n",
      "4        ctx-rh-parsopercularis            10            16\n",
      "14             Left-Hippocampus            10             8\n",
      "24      ctx-rh-inferiorparietal            10            17\n",
      "20      ctx-rh-parstriangularis            10            11\n",
      "19            Right-Hippocampus             9             5\n",
      "27         ctx-lh-supramarginal             9            13\n",
      "7          ctx-rh-supramarginal             9            16\n",
      "13      ctx-lh-inferiorparietal             9             5\n",
      "11      ctx-rh-lateraloccipital             9            12\n",
      "10   ctx-rh-caudalmiddlefrontal             7            14\n",
      "17           ctx-lh-postcentral             7             7\n",
      "1             ctx-lh-precentral             7            14\n",
      "30       ctx-lh-parsopercularis             7            12\n",
      "23              ctx-rh-bankssts             6             5\n",
      "25       ctx-rh-superiorfrontal             6            16\n",
      "26   ctx-lh-medialorbitofrontal             6            11\n",
      "6       ctx-rh-superiorparietal             5            12\n",
      "15      ctx-rh-superiortemporal             5            15\n",
      "31   ctx-lh-caudalmiddlefrontal             5             8\n",
      "0    ctx-rh-medialorbitofrontal             5            10\n"
     ]
    }
   ],
   "source": [
    "def analyze_regional_coverage():\n",
    "    \"\"\"\n",
    "    Analyze regional coverage and patient counts in both cohorts\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    hup_features = pd.read_csv(\"../results/ge_go_hup_region_features.csv\")\n",
    "    mni_features = pd.read_csv(\"../results/mni_region_features.csv\")\n",
    "    \n",
    "    print(\"\\nHUP Dataset Analysis:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Get patient and region counts for HUP\n",
    "    hup_regions = hup_features.groupby('roi').agg({\n",
    "        'patient_id': ['nunique', 'count']\n",
    "    }).reset_index()\n",
    "    hup_regions.columns = ['roi', 'n_patients', 'n_recordings']\n",
    "    hup_regions = hup_regions.sort_values('n_patients', ascending=False)\n",
    "    \n",
    "    print(f\"\\nTotal unique regions in HUP: {len(hup_regions)}\")\n",
    "    print(f\"Total unique patients in HUP: {hup_features['patient_id'].nunique()}\")\n",
    "    print(\"\\nPatient counts per region (top 10):\")\n",
    "    print(hup_regions.head(10))\n",
    "    \n",
    "    print(\"\\nRegions with < 5 patients:\")\n",
    "    print(hup_regions[hup_regions['n_patients'] < 5][['roi', 'n_patients']])\n",
    "    \n",
    "    print(\"\\nMNI Dataset Analysis:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Get patient and region counts for MNI\n",
    "    mni_regions = mni_features.groupby('roi').agg({\n",
    "        'patient_id': ['nunique', 'count']\n",
    "    }).reset_index()\n",
    "    mni_regions.columns = ['roi', 'n_patients', 'n_recordings']\n",
    "    mni_regions = mni_regions.sort_values('n_patients', ascending=False)\n",
    "    \n",
    "    print(f\"\\nTotal unique regions in MNI: {len(mni_regions)}\")\n",
    "    print(f\"Total unique patients in MNI: {mni_features['patient_id'].nunique()}\")\n",
    "    print(\"\\nPatient counts per region (top 10):\")\n",
    "    print(mni_regions.head(10))\n",
    "    \n",
    "    print(\"\\nRegions with < 5 patients:\")\n",
    "    print(mni_regions[mni_regions['n_patients'] < 5][['roi', 'n_patients']])\n",
    "    \n",
    "    # Analyze overlap\n",
    "    common_regions = set(hup_regions['roi']) & set(mni_regions['roi'])\n",
    "    print(f\"\\nOverlap Analysis:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Number of common regions: {len(common_regions)}\")\n",
    "    \n",
    "    # Analyze which regions have sufficient patients in both cohorts\n",
    "    valid_regions = []\n",
    "    for region in common_regions:\n",
    "        hup_patients = hup_regions[hup_regions['roi'] == region]['n_patients'].iloc[0]\n",
    "        mni_patients = mni_regions[mni_regions['roi'] == region]['n_patients'].iloc[0]\n",
    "        \n",
    "        if hup_patients >= 5 and mni_patients >= 5:\n",
    "            valid_regions.append({\n",
    "                'region': region,\n",
    "                'hup_patients': hup_patients,\n",
    "                'mni_patients': mni_patients\n",
    "            })\n",
    "    \n",
    "    print(f\"\\nRegions with ≥5 patients in both cohorts: {len(valid_regions)}\")\n",
    "    print(\"\\nValid regions for paired analysis:\")\n",
    "    valid_df = pd.DataFrame(valid_regions)\n",
    "    print(valid_df.sort_values('hup_patients', ascending=False))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_regional_coverage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistical Analysis Methodology:\n",
      "==================================================\n",
      "\n",
      "1. Sample Size Analysis:\n",
      "   - HUP Cohort: 28 unique patients across 67 regions\n",
      "   - MNI Cohort: 106 unique patients across 68 regions\n",
      "   - 57 overlapping regions between cohorts\n",
      "   - 33 regions had sufficient data (≥5 patients in both cohorts)\n",
      "\n",
      "2. Statistical Testing Pipeline:\n",
      "   Current Implementation:\n",
      "   a) Data preparation:\n",
      "      - Group by patient within each region\n",
      "      - Match patient counts between cohorts\n",
      "      - Minimum requirement: 5 patients per group\n",
      "\n",
      "   b) For each region and feature:\n",
      "      - Wilcoxon signed-rank test (paired, two-tailed)\n",
      "      - Effect size computation (Cohen's d for paired data)\n",
      "      - FDR correction for multiple comparisons\n",
      "\n",
      "3. Potential Improvements Needed:\n",
      "   a) Patient Matching:\n",
      "      - Current implementation truncates to equal lengths\n",
      "      - Should use consistent patient subset across features\n",
      "   b) Multiple Comparison Correction:\n",
      "      - Currently using FDR across all tests\n",
      "      - Consider hierarchical testing strategy:\n",
      "        * First test global differences\n",
      "        * Then test individual regions if global is significant\n",
      "   c) Effect Size Interpretation:\n",
      "      - Add confidence intervals for effect sizes\n",
      "\n",
      "4. Interpretation of Current Results:\n",
      "   - No regions showed significant differences after correction\n",
      "   - Possible explanations:\n",
      "     a) True absence of site-specific differences\n",
      "     b) Insufficient power due to small sample sizes\n",
      "     c) High variability within regions\n",
      "     d) Over-conservative multiple comparison correction\n"
     ]
    }
   ],
   "source": [
    "def print_analysis_methodology():\n",
    "    \"\"\"\n",
    "    Print detailed summary of statistical methodology and findings in the console\n",
    "    \"\"\"\n",
    "    print(\"\\nStatistical Analysis Methodology:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"\\n1. Sample Size Analysis:\")\n",
    "    print(\"   - HUP Cohort: 28 unique patients across 67 regions\")\n",
    "    print(\"   - MNI Cohort: 106 unique patients across 68 regions\")\n",
    "    print(\"   - 57 overlapping regions between cohorts\")\n",
    "    print(\"   - 33 regions had sufficient data (≥5 patients in both cohorts)\")\n",
    "    \n",
    "    print(\"\\n2. Statistical Testing Pipeline:\")\n",
    "    print(\"   Current Implementation:\")\n",
    "    print(\"   a) Data preparation:\")\n",
    "    print(\"      - Group by patient within each region\")\n",
    "    print(\"      - Match patient counts between cohorts\")\n",
    "    print(\"      - Minimum requirement: 5 patients per group\")\n",
    "    \n",
    "    print(\"\\n   b) For each region and feature:\")\n",
    "    print(\"      - Wilcoxon signed-rank test (paired, two-tailed)\")\n",
    "    print(\"      - Effect size computation (Cohen's d for paired data)\")\n",
    "    print(\"      - FDR correction for multiple comparisons\")\n",
    "    \n",
    "    print(\"\\n3. Potential Improvements Needed:\")\n",
    "    print(\"   a) Patient Matching:\")\n",
    "    print(\"      - Current implementation truncates to equal lengths\")\n",
    "    print(\"      - Should use consistent patient subset across features\")\n",
    "    \n",
    "    print(\"   b) Multiple Comparison Correction:\")\n",
    "    print(\"      - Currently using FDR across all tests\")\n",
    "    print(\"      - Consider hierarchical testing strategy:\")\n",
    "    print(\"        * First test global differences\")\n",
    "    print(\"        * Then test individual regions if global is significant\")\n",
    "    \n",
    "    print(\"   c) Effect Size Interpretation:\")\n",
    "    print(\"      - Add confidence intervals for effect sizes\")\n",
    "    \n",
    "    print(\"\\n4. Interpretation of Current Results:\")\n",
    "    print(\"   - No regions showed significant differences after correction\")\n",
    "    print(\"   - Possible explanations:\")\n",
    "    print(\"     a) True absence of site-specific differences\")\n",
    "    print(\"     b) Insufficient power due to small sample sizes\")\n",
    "    print(\"     c) High variability within regions\")\n",
    "    print(\"     d) Over-conservative multiple comparison correction\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print_analysis_methodology()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnt_gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
