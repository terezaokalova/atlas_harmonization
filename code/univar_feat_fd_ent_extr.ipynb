{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import joblib \n",
    "import dill \n",
    "\n",
    "import mne\n",
    "from scipy.signal import welch, get_window\n",
    "from scipy.signal.windows import hamming\n",
    "\n",
    "from scipy.signal import butter, lfilter, sosfilt, filtfilt, sosfreqz, iirnotch\n",
    "from scipy import fftpack\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path_data = '../Data'\n",
    "base_path_results = '../results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TIME SERIES DATA\n",
    "\n",
    "# dict_keys(['__header__', '__version__', '__globals__', 'SamplingFrequency', 'depth_elecs', 'mni_coords', 'patient_no', 'resected_ch', 'soz_ch', 'spike_24h', 'wake_clip'])\n",
    "hup_atlas = sio.loadmat(os.path.join(base_path_data, 'HUP_atlas.mat'))\n",
    "# dict_keys(['__header__', '__version__', '__globals__', 'AgeAtTimeOfStudy', 'ChannelName', 'ChannelPosition', 'ChannelRegion', 'ChannelType', \n",
    "# 'Data_N2', 'Data_N3', 'Data_R', 'Data_W', 'FacesLeft', 'FacesRight', 'Gender', 'Hemisphere', 'NodesLeft', 'NodesLeftInflated', 'NodesRegionLeft', \n",
    "# 'NodesRegionRight', 'NodesRight', 'NodesRightInflated', 'Patient', 'RegionName', 'SamplingFrequency'])\n",
    "mni_atlas = sio.loadmat(os.path.join(base_path_data, 'MNI_atlas.mat'))\n",
    "\n",
    "# (12000, 3431) @ x-axis is time steps, y-axis is electrodes\n",
    "hup_ts = pd.DataFrame(hup_atlas['wake_clip'])\n",
    "# (13600, 1765)  @ x-axis is time steps, y-axis is electrodes\n",
    "mni_ts = pd.DataFrame(mni_atlas['Data_W'])\n",
    "\n",
    "# accessing columns for patients\n",
    "hup_patients = pd.DataFrame(hup_atlas['patient_no'])\n",
    "mni_patients = pd.DataFrame(mni_atlas['Patient'])\n",
    "\n",
    "# electrode counts\n",
    "hup_patient_total_el_counts = len(hup_atlas['patient_no'])\n",
    "mni_patient_total_el_counts = len(mni_atlas['Patient'])\n",
    "\n",
    "# unique patient ids\n",
    "hup_patient_ids = np.unique(hup_atlas['patient_no'])\n",
    "mni_patient_ids = np.unique(mni_atlas['Patient']) \n",
    "\n",
    "# sampling frequency\n",
    "mni_samp_freq = int(mni_atlas['SamplingFrequency'].flatten()[~np.isnan(mni_atlas['SamplingFrequency'].flatten())][0])\n",
    "hup_samp_freq = int(hup_atlas['SamplingFrequency'].flatten()[~np.isnan(hup_atlas['SamplingFrequency'].flatten())][0])\n",
    "\n",
    "# mapping electrodes to their respective patients\n",
    "hup_patient_numbers = hup_atlas['patient_no'].flatten()\n",
    "hup_el_to_pat_map_dict = {}\n",
    "for idx, patient_num in enumerate(hup_patient_numbers):\n",
    "    hup_el_to_pat_map_dict[idx] = patient_num\n",
    "hup_idx_map_arr = np.array([patient_num for patient_num in hup_patient_numbers]) # arr equivalent\n",
    "\n",
    "mni_patient_numbers = mni_atlas['Patient'].flatten()\n",
    "mni_el_to_pat_map_dict = {}\n",
    "for idx, patient_num in enumerate(mni_patient_numbers):\n",
    "    mni_el_to_pat_map_dict[idx] = patient_num\n",
    "mni_idx_map_arr = np.array([patient_num for patient_num in mni_patient_numbers])\n",
    "\n",
    "## REGION MAPS\n",
    "\n",
    "dk_atlas_df = pd.read_csv(os.path.join(base_path_data, 'desikanKilliany.csv'))\n",
    "# columns: Index(['x', 'y', 'z', 'roiNum', 'snum', 'abvr', 'lobe', 'isSideLeft'], dtype='object')\n",
    "hup_df = pd.read_csv(os.path.join(base_path_data, 'hup_df.csv'))\n",
    "# columns: Index(['x', 'y', 'z', 'roiNum', 'snum', 'abvr', 'lobe', 'isSideLeft'], dtype='object')\n",
    "mni_df = pd.read_csv(os.path.join(base_path_data, 'mni_df.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# already saved\n",
    "# hup_idx_map_arr_df = pd.DataFrame(hup_idx_map_arr)\n",
    "# hup_idx_map_arr_df.to_csv(os.path.join(base_path_results, 'hup_idx_map_arr.csv'))\n",
    "# mni_idx_map_arr_df = pd.DataFrame(mni_idx_map_arr)\n",
    "# mni_idx_map_arr_df.to_csv(os.path.join(base_path_results, 'mni_idx_map_arr.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norm_psd(iEEGnormal, data_timeS, sampling_frequency=200):\n",
    "    \"\"\"\n",
    "    Function to compute normalized power spectral densities for different EEG frequency bands.\n",
    "    \n",
    "    Args:\n",
    "    iEEGnormal (DataFrame): A DataFrame to append results to.\n",
    "    data_timeS (array): Time domain EEG data for a single electrode (1D array)\n",
    "    sampling_frequency (int): Sampling frequency of the EEG data.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: Updated DataFrame with new EEG features.\n",
    "    \"\"\"\n",
    "    \n",
    "    Fs = sampling_frequency\n",
    "    window = Fs * 2\n",
    "    NFFT = window\n",
    "    \n",
    "    # Compute PSD\n",
    "    f, data_psd = welch(data_timeS, fs=Fs, window=hamming(window), \n",
    "                       nfft=NFFT, scaling='density', noverlap=window//2)\n",
    "    \n",
    "    # filter out noise frequency 57.5Hz to 62.5Hz\n",
    "    noise_mask = (f >= 57.5) & (f <= 62.5)\n",
    "    f = f[~noise_mask]\n",
    "    # Handle 1D data_psd\n",
    "    data_psd = data_psd[~noise_mask]\n",
    "    \n",
    "    def bandpower(psd, freqs, freq_range):\n",
    "        \"\"\"Calculate power in the given frequency range.\"\"\"\n",
    "        idx = np.logical_and(freqs >= freq_range[0], freqs <= freq_range[1])\n",
    "        return np.trapezoid(psd[idx], freqs[idx]) \n",
    "    \n",
    "    # Define frequency bands\n",
    "    bands = {'delta': (1, 4), 'theta': (4, 8), 'alpha': (8, 13), \n",
    "             'beta': (13, 30), 'gamma': (30, 80), 'broad': (1, 80)}\n",
    "    \n",
    "    # Calculate band powers (using 1D data_psd)\n",
    "    band_powers = {band: bandpower(data_psd, f, freq_range) \n",
    "                  for band, freq_range in bands.items()}\n",
    "    \n",
    "    # Compute log transform\n",
    "    log_band_powers = {f'{band}log': np.log10(power + 1) \n",
    "                      for band, power in band_powers.items()}\n",
    "    \n",
    "    # Calculate total power\n",
    "    total_band_power = np.sum([value for value in log_band_powers.values()])\n",
    "    \n",
    "    # Calculate relative powers\n",
    "    relative_band_powers = {f'{band}Rel': log_band_powers[f'{band}log'] / total_band_power \n",
    "                          for band in bands}\n",
    "    \n",
    "    # Create DataFrame row\n",
    "    data_to_append = pd.DataFrame([relative_band_powers])\n",
    "    # data_to_append['broadlog'] = log_band_powers['broadlog']\n",
    "    \n",
    "    # Append to existing DataFrame\n",
    "    iEEGnormal = pd.concat([iEEGnormal, data_to_append], ignore_index=True)\n",
    "    \n",
    "    return iEEGnormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not actually used\n",
    "def compute_shannon_entropy(signal):\n",
    "    \"\"\"\n",
    "    Compute Shannon entropy from EEG signal\n",
    "    \n",
    "    Parameters:\n",
    "    signal: 1D array of EEG values\n",
    "    \n",
    "    Returns:\n",
    "    float: Shannon entropy value\n",
    "    \"\"\"\n",
    "    # 1. Estimate probability distribution using histogram\n",
    "    hist, bin_edges = np.histogram(signal, bins='auto', density=True)\n",
    "    \n",
    "    # 2. Normalize to ensure probabilities sum to 1\n",
    "    probabilities = hist / hist.sum()\n",
    "    \n",
    "    # 3. Remove any zero probabilities (since log(0) is undefined)\n",
    "    probabilities = probabilities[probabilities > 0]\n",
    "    \n",
    "    # 4. Compute Shannon entropy\n",
    "    H = -np.sum(probabilities * np.log2(probabilities))\n",
    "    \n",
    "    # 5. Log transform (optional, for feature scaling)\n",
    "    H_log = np.log10(H + 1)\n",
    "    \n",
    "    return H_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norm_entropy_full_ts(ieeg_normal, data_time_s, sampling_frequency=200, window_size_mins=1, stride_mins=0.5):\n",
    "   \"\"\"\n",
    "   Compute entropy across full time series using sliding windows.\n",
    "   \n",
    "   Args:\n",
    "       ieeg_normal: DataFrame to append results to\n",
    "       data_time_s: Time series data\n",
    "       sampling_frequency: Sampling rate of the data\n",
    "       window_size_mins: Size of sliding window in minutes\n",
    "       stride_mins: Stride between windows in minutes\n",
    "   \n",
    "   Returns:\n",
    "       DataFrame with computed entropy values\n",
    "   \"\"\"\n",
    "   if data_time_s.ndim == 1:\n",
    "       data_time_s = data_time_s.reshape(-1,1)\n",
    "   \n",
    "   # Calculate window parameters\n",
    "   samples_per_window = sampling_frequency * 60 * window_size_mins\n",
    "   stride_samples = sampling_frequency * 60 * stride_mins\n",
    "   n_windows = int((len(data_time_s) - samples_per_window) // stride_samples + 1)\n",
    "   \n",
    "   entropies = []\n",
    "   for i in range(n_windows):\n",
    "       start_idx = int(i * stride_samples)\n",
    "       end_idx = int(start_idx + samples_per_window)\n",
    "       window_data = data_time_s[start_idx:end_idx, :]\n",
    "       \n",
    "       # Apply filters\n",
    "       b, a = butter(3, 80/(sampling_frequency/2), btype='low')\n",
    "       filtered = filtfilt(b, a, window_data.astype(float), axis=0)\n",
    "       \n",
    "       b, a = butter(3, 1/(sampling_frequency/2), btype='high')\n",
    "       filtered = filtfilt(b, a, filtered, axis=0)\n",
    "       \n",
    "       b, a = iirnotch(60, 30, sampling_frequency)\n",
    "       filtered = filtfilt(b, a, filtered, axis=0)\n",
    "       \n",
    "       # Compute entropy for this window\n",
    "       signal = filtered[:, 0]\n",
    "       \n",
    "       # Estimate probability distribution\n",
    "       hist, _ = np.histogram(signal, bins='auto', density=True)\n",
    "       probabilities = hist / hist.sum()\n",
    "       probabilities = probabilities[probabilities > 0]\n",
    "       \n",
    "       # Compute Shannon entropy\n",
    "       H = -np.sum(probabilities * np.log2(probabilities))\n",
    "       entropies.append(H)\n",
    "   \n",
    "   # Store raw entropy values\n",
    "   data_to_append = pd.DataFrame({\n",
    "       'entropy_fullts': [np.log10(np.mean(entropies) + 1)]  # Log transform of raw entropy values\n",
    "   })\n",
    "   \n",
    "   return pd.concat([ieeg_normal, data_to_append], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norm_entropy_1min_seg(ieeg_normal, data_time_s, sampling_frequency=200):\n",
    "    if data_time_s.ndim == 1:\n",
    "        data_time_s = data_time_s.reshape(-1,1)\n",
    "    \n",
    "    # Get first minute of data\n",
    "    data_seg = data_time_s[:sampling_frequency*60, :]\n",
    "    \n",
    "    # Low pass filter at 80Hz\n",
    "    b, a = butter(3, 80/(sampling_frequency/2), btype='low')\n",
    "    data_seg_filtered = filtfilt(b, a, data_seg.astype(float), axis=0)\n",
    "    \n",
    "    # High pass filter at 1Hz  \n",
    "    b, a = butter(3, 1/(sampling_frequency/2), btype='high')\n",
    "    data_seg_filtered = filtfilt(b, a, data_seg_filtered, axis=0)\n",
    "    \n",
    "    # Notch filter at 60Hz\n",
    "    b, a = iirnotch(60, 30, sampling_frequency)\n",
    "    data_seg_notch = filtfilt(b, a, data_seg_filtered, axis=0)\n",
    "\n",
    "    # Compute Shannon entropy for each channel\n",
    "    data_entropy = np.zeros((data_seg_notch.shape[1], 1))\n",
    "    for chan in range(data_seg_notch.shape[1]):\n",
    "        signal = data_seg_notch[:, chan]\n",
    "        # Estimate probability distribution\n",
    "        hist, _ = np.histogram(signal, bins='auto', density=True)\n",
    "        hist = hist / hist.sum()\n",
    "        data_entropy[chan] = entropy(hist)\n",
    "    \n",
    "    # Log transform of non-negative entropy\n",
    "    data_entropy = np.log10(data_entropy + 1)\n",
    "    \n",
    "    # Create new row\n",
    "    data_to_append = pd.DataFrame({'entropy': data_entropy.flatten()})\n",
    "    \n",
    "    return pd.concat([ieeg_normal, data_to_append], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "hup_psd_features = pd.DataFrame()\n",
    "hup_entropy_1min_features = pd.DataFrame()\n",
    "hup_entropy_fullts_features = pd.DataFrame()\n",
    "\n",
    "# Process HUP data\n",
    "for patient in hup_patient_ids:\n",
    "   patient_el_ids = np.where(hup_idx_map_arr == patient)[0]\n",
    "   \n",
    "   for idx in patient_el_ids:\n",
    "       electrode_data = hup_ts.iloc[:, idx].values\n",
    "       hup_psd_features = get_norm_psd(hup_psd_features, electrode_data)\n",
    "       hup_entropy_1min_features = get_norm_entropy_1min_seg(hup_entropy_1min_features, electrode_data)\n",
    "       hup_entropy_fullts_features = get_norm_entropy_full_ts(hup_entropy_fullts_features, electrode_data)\n",
    "\n",
    "# Combine HUP features\n",
    "hup_features = pd.concat([\n",
    "   hup_psd_features,\n",
    "   hup_entropy_1min_features[['entropy']].rename(columns={'entropy': 'entropy_1min'}),\n",
    "   hup_entropy_fullts_features[['entropy_fullts']], # Updated column name\n",
    "], axis=1)\n",
    "\n",
    "# Initialize MNI DataFrames\n",
    "mni_psd_features = pd.DataFrame()\n",
    "mni_entropy_1min_features = pd.DataFrame()\n",
    "mni_entropy_fullts_features = pd.DataFrame()\n",
    "\n",
    "# Process MNI data\n",
    "for patient in mni_patient_ids:\n",
    "   patient_el_ids = np.where(mni_idx_map_arr == patient)[0]\n",
    "   \n",
    "   for idx in patient_el_ids:\n",
    "       electrode_data = mni_ts.iloc[:, idx].values\n",
    "       mni_psd_features = get_norm_psd(mni_psd_features, electrode_data)\n",
    "       mni_entropy_1min_features = get_norm_entropy_1min_seg(mni_entropy_1min_features, electrode_data)\n",
    "       mni_entropy_fullts_features = get_norm_entropy_full_ts(mni_entropy_fullts_features, electrode_data)\n",
    "\n",
    "# Combine MNI features\n",
    "mni_features = pd.concat([\n",
    "   mni_psd_features,\n",
    "   mni_entropy_1min_features[['entropy']].rename(columns={'entropy': 'entropy_1min'}),\n",
    "   mni_entropy_fullts_features[['entropy_fullts']], \n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUP Features shape: (3431, 8)\n",
      "HUP Features columns: ['deltaRel', 'thetaRel', 'alphaRel', 'betaRel', 'gammaRel', 'broadRel', 'entropy_1min', 'entropy_fullts']\n",
      "\n",
      "MNI Features shape: (1765, 8)\n",
      "MNI Features columns: ['deltaRel', 'thetaRel', 'alphaRel', 'betaRel', 'gammaRel', 'broadRel', 'entropy_1min', 'entropy_fullts']\n"
     ]
    }
   ],
   "source": [
    "print(\"HUP Features shape:\", hup_features.shape)\n",
    "print(\"HUP Features columns:\", hup_features.columns.tolist())\n",
    "print(\"\\nMNI Features shape:\", mni_features.shape)\n",
    "print(\"MNI Features columns:\", mni_features.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUP Features:\n",
      "Shape: (3431, 8)\n",
      "Columns: ['deltaRel', 'thetaRel', 'alphaRel', 'betaRel', 'gammaRel', 'broadRel', 'entropy_1min', 'entropy_fullts']\n",
      "\n",
      "First few rows:\n",
      "   deltaRel  thetaRel  alphaRel   betaRel  gammaRel  broadRel  entropy_1min  \\\n",
      "0  0.177924  0.147311  0.135588  0.173211  0.139114  0.226852      0.660497   \n",
      "1  0.210691  0.231001  0.098005  0.129331  0.048090  0.282882      0.664082   \n",
      "2  0.248122  0.184206  0.116251  0.126453  0.059346  0.265622      0.662271   \n",
      "3  0.214907  0.162279  0.114287  0.135578  0.073299  0.299651      0.659488   \n",
      "4  0.187794  0.182233  0.136686  0.152295  0.069230  0.271761      0.657976   \n",
      "\n",
      "   entropy_fullts  \n",
      "0        0.789527  \n",
      "1        0.793369  \n",
      "2        0.791429  \n",
      "3        0.788446  \n",
      "4        0.786825  \n",
      "\n",
      "Basic statistics:\n",
      "           deltaRel     thetaRel     alphaRel      betaRel     gammaRel  \\\n",
      "count  3431.000000  3431.000000  3431.000000  3431.000000  3431.000000   \n",
      "mean      0.195423     0.172769     0.153454     0.148689     0.098853   \n",
      "std       0.031672     0.018485     0.022784     0.024257     0.029283   \n",
      "min       0.079483     0.067368     0.030700     0.035900     0.014172   \n",
      "25%       0.173443     0.162974     0.141779     0.135374     0.080027   \n",
      "50%       0.192697     0.172821     0.152823     0.151037     0.102781   \n",
      "75%       0.212900     0.182852     0.164543     0.163997     0.119706   \n",
      "max       0.376323     0.277389     0.294002     0.292550     0.177618   \n",
      "\n",
      "          broadRel  entropy_1min  entropy_fullts  \n",
      "count  3431.000000   3431.000000     3431.000000  \n",
      "mean      0.230813      0.667202        0.796707  \n",
      "std       0.027647      0.006298        0.006737  \n",
      "min       0.196556      0.651728        0.780122  \n",
      "25%       0.213350      0.662786        0.791980  \n",
      "50%       0.222750      0.665913        0.795330  \n",
      "75%       0.239422      0.670302        0.800029  \n",
      "max       0.464221      0.748925        0.883668   \n",
      "\n",
      "\n",
      "MNI Features:\n",
      "Shape: (1765, 8)\n",
      "Columns: ['deltaRel', 'thetaRel', 'alphaRel', 'betaRel', 'gammaRel', 'broadRel', 'entropy_1min', 'entropy_fullts']\n",
      "\n",
      "First few rows:\n",
      "   deltaRel  thetaRel  alphaRel   betaRel  gammaRel  broadRel  entropy_1min  \\\n",
      "0  0.157038  0.143701  0.155727  0.200617  0.096841  0.246077      0.665212   \n",
      "1  0.166424  0.141783  0.145537  0.198588  0.105052  0.242617      0.665628   \n",
      "2  0.162231  0.146422  0.150696  0.195236  0.111818  0.233596      0.667045   \n",
      "3  0.169508  0.139919  0.160346  0.205149  0.092476  0.232602      0.672854   \n",
      "4  0.171363  0.157544  0.150542  0.159429  0.133543  0.227579      0.660791   \n",
      "\n",
      "   entropy_fullts  \n",
      "0        0.794580  \n",
      "1        0.795025  \n",
      "2        0.796542  \n",
      "3        0.802759  \n",
      "4        0.789843  \n",
      "\n",
      "Basic statistics:\n",
      "           deltaRel     thetaRel     alphaRel      betaRel     gammaRel  \\\n",
      "count  1765.000000  1765.000000  1765.000000  1765.000000  1765.000000   \n",
      "mean      0.178371     0.168981     0.157086     0.163744     0.100066   \n",
      "std       0.024600     0.020741     0.022248     0.022897     0.029884   \n",
      "min       0.085966     0.087047     0.074283     0.069782     0.013149   \n",
      "25%       0.162217     0.155562     0.142463     0.150639     0.080891   \n",
      "50%       0.177869     0.166918     0.155314     0.162416     0.103560   \n",
      "75%       0.192630     0.179845     0.171038     0.176666     0.121759   \n",
      "max       0.296321     0.269815     0.251089     0.289101     0.194566   \n",
      "\n",
      "          broadRel  entropy_1min  entropy_fullts  \n",
      "count  1765.000000   1765.000000     1765.000000  \n",
      "mean      0.231752      0.665009        0.794360  \n",
      "std       0.020353      0.004354        0.004662  \n",
      "min       0.202262      0.652029        0.780445  \n",
      "25%       0.217895      0.661918        0.791051  \n",
      "50%       0.227018      0.664129        0.793419  \n",
      "75%       0.240215      0.667155        0.796660  \n",
      "max       0.334327      0.685827        0.816622   \n",
      "\n",
      "Missing values in HUP: deltaRel          0\n",
      "thetaRel          0\n",
      "alphaRel          0\n",
      "betaRel           0\n",
      "gammaRel          0\n",
      "broadRel          0\n",
      "entropy_1min      0\n",
      "entropy_fullts    0\n",
      "dtype: int64\n",
      "Missing values in MNI: deltaRel          0\n",
      "thetaRel          0\n",
      "alphaRel          0\n",
      "betaRel           0\n",
      "gammaRel          0\n",
      "broadRel          0\n",
      "entropy_1min      0\n",
      "entropy_fullts    0\n",
      "dtype: int64\n",
      "\n",
      "HUP Feature Ranges:\n",
      "deltaRel: [0.079, 0.376]\n",
      "thetaRel: [0.067, 0.277]\n",
      "alphaRel: [0.031, 0.294]\n",
      "betaRel: [0.036, 0.293]\n",
      "gammaRel: [0.014, 0.178]\n",
      "broadRel: [0.197, 0.464]\n",
      "entropy_1min: [0.652, 0.749]\n",
      "entropy_fullts: [0.780, 0.884]\n",
      "\n",
      "MNI Feature Ranges:\n",
      "deltaRel: [0.086, 0.296]\n",
      "thetaRel: [0.087, 0.270]\n",
      "alphaRel: [0.074, 0.251]\n",
      "betaRel: [0.070, 0.289]\n",
      "gammaRel: [0.013, 0.195]\n",
      "broadRel: [0.202, 0.334]\n",
      "entropy_1min: [0.652, 0.686]\n",
      "entropy_fullts: [0.780, 0.817]\n"
     ]
    }
   ],
   "source": [
    "# Basic shape and info\n",
    "print(\"HUP Features:\")\n",
    "print(f\"Shape: {hup_features.shape}\")\n",
    "print(f\"Columns: {hup_features.columns.tolist()}\\n\")\n",
    "print(f\"First few rows:\\n{hup_features.head()}\\n\")\n",
    "print(\"Basic statistics:\\n\", hup_features.describe(), \"\\n\")\n",
    "\n",
    "print(\"\\nMNI Features:\")\n",
    "print(f\"Shape: {mni_features.shape}\")\n",
    "print(f\"Columns: {mni_features.columns.tolist()}\\n\")\n",
    "print(f\"First few rows:\\n{mni_features.head()}\\n\")\n",
    "print(\"Basic statistics:\\n\", mni_features.describe(), \"\\n\")\n",
    "\n",
    "# Check for any missing values\n",
    "print(\"Missing values in HUP:\", hup_features.isnull().sum())\n",
    "print(\"Missing values in MNI:\", mni_features.isnull().sum())\n",
    "\n",
    "# Value ranges for each feature\n",
    "print(\"\\nHUP Feature Ranges:\")\n",
    "for col in hup_features.columns:\n",
    "    print(f\"{col}: [{hup_features[col].min():.3f}, {hup_features[col].max():.3f}]\")\n",
    "\n",
    "print(\"\\nMNI Feature Ranges:\")\n",
    "for col in mni_features.columns:\n",
    "    print(f\"{col}: [{mni_features[col].min():.3f}, {mni_features[col].max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save combined features (electrode-wise)\n",
    "# hup_features.to_csv(os.path.join(base_path_results, 'hup_univar_feats_eld.csv'), index=False)\n",
    "# mni_features.to_csv(os.path.join(base_path_results, 'mni_univar_feats_eld.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggreggate features based on ROI mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_features_by_region(features_df, region_df, dk_atlas_df, patient_map_arr):\n",
    "    \"\"\"\n",
    "    Aggregate electrode-level features to region-level features while preserving patient identity\n",
    "    \n",
    "    Args:\n",
    "        features_df: DataFrame with electrode features\n",
    "        region_df: DataFrame with electrode to region mapping\n",
    "        dk_atlas_df: DataFrame with region information\n",
    "        patient_map_arr: Array mapping electrodes to patient IDs\n",
    "    \"\"\"\n",
    "    # Add patient ID and region information to features\n",
    "    combined_df = pd.concat([\n",
    "        features_df.reset_index(drop=True),\n",
    "        pd.DataFrame({\n",
    "            'roiNum': region_df['roiNum'].reset_index(drop=True),\n",
    "            'patient_id': patient_map_arr\n",
    "        })\n",
    "    ], axis=1)\n",
    "    \n",
    "    # Define features to aggregate\n",
    "    spectral_features = ['deltaRel', 'thetaRel', 'alphaRel', 'betaRel', 'gammaRel']\n",
    "    entropy_features = ['entropy_1min', 'entropy_fullts']\n",
    "    \n",
    "    # Group by both patient and region\n",
    "    region_features = []\n",
    "    \n",
    "    for (pat_id, roi), group in combined_df.groupby(['patient_id', 'roiNum']):\n",
    "        row_dict = {\n",
    "            'patient_id': pat_id,\n",
    "            'roiNum': roi\n",
    "        }\n",
    "        \n",
    "        # Calculate means for each feature\n",
    "        for feat in spectral_features + entropy_features:\n",
    "            if feat in group.columns:\n",
    "                row_dict[f\"{feat}_mean\"] = group[feat].mean()\n",
    "        \n",
    "        region_features.append(row_dict)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    region_features_df = pd.DataFrame(region_features)\n",
    "    \n",
    "    # Add region names\n",
    "    region_features_df = pd.merge(\n",
    "        region_features_df,\n",
    "        dk_atlas_df[['roiNum', 'roi']].drop_duplicates('roiNum'),\n",
    "        on='roiNum',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    return region_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_and_average_by_region(features_df, region_df, dk_atlas_df, patient_map_arr):\n",
    "    \"\"\"\n",
    "    Average features across all electrodes in each region\n",
    "    Arguments added for region name mapping and patient tracking\n",
    "    \"\"\"\n",
    "    combined_df = pd.concat([\n",
    "        features_df.reset_index(drop=True),\n",
    "        pd.DataFrame({\n",
    "            'roiNum': region_df['roiNum'].reset_index(drop=True),\n",
    "            'patient_id': patient_map_arr\n",
    "        })\n",
    "    ], axis=1)\n",
    "    \n",
    "    # First average within patient-region\n",
    "    patient_region_avg = combined_df.groupby(['patient_id', 'roiNum']).mean()\n",
    "    \n",
    "    # Then average across patients for each region\n",
    "    region_avg = patient_region_avg.groupby('roiNum').mean()\n",
    "    \n",
    "    # Add region names\n",
    "    region_avg = pd.merge(\n",
    "        region_avg,\n",
    "        dk_atlas_df[['roiNum', 'roi']].drop_duplicates('roiNum'),\n",
    "        on='roiNum',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    return region_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "hup_univar_feats = hup_features\n",
    "mni_univar_feats = mni_features\n",
    "\n",
    "# Process HUP data\n",
    "hup_region_features = aggregate_features_by_region(\n",
    "    hup_univar_feats, \n",
    "    hup_df, \n",
    "    dk_atlas_df, \n",
    "    hup_idx_map_arr\n",
    ")\n",
    "\n",
    "hup_region_avg_feats = aggregate_and_average_by_region(\n",
    "    hup_univar_feats, \n",
    "    hup_df, \n",
    "    dk_atlas_df, \n",
    "    hup_idx_map_arr\n",
    ")\n",
    "\n",
    "# Process MNI data\n",
    "mni_region_features = aggregate_features_by_region(\n",
    "    mni_univar_feats, \n",
    "    mni_df, \n",
    "    dk_atlas_df, \n",
    "    mni_idx_map_arr\n",
    ")\n",
    "\n",
    "mni_region_avg_feats = aggregate_and_average_by_region(\n",
    "    mni_univar_feats, \n",
    "    mni_df, \n",
    "    dk_atlas_df, \n",
    "    mni_idx_map_arr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['patient_id', 'roiNum', 'deltaRel_mean', 'thetaRel_mean',\n",
      "       'alphaRel_mean', 'betaRel_mean', 'gammaRel_mean', 'entropy_1min_mean',\n",
      "       'entropy_fullts_mean', 'roi'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# print all the features stored in hup_region_features\n",
    "print(hup_region_features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUP Dataset Verification:\n",
      "-----------------------\n",
      "Total number of rows: 1064\n",
      "Unique patients: 60\n",
      "Unique regions: 76\n",
      "\n",
      "Number of regions per HUP patient:\n",
      "count    60.000000\n",
      "mean     17.733333\n",
      "std       5.724958\n",
      "min       5.000000\n",
      "25%      14.000000\n",
      "50%      18.000000\n",
      "75%      21.000000\n",
      "max      32.000000\n",
      "Name: roi, dtype: float64\n",
      "\n",
      "Example - Patient with most regions:\n",
      "\n",
      "Patient 48 regions:\n",
      "                             roi  deltaRel_mean  entropy_1min_mean\n",
      "784             Left-Hippocampus       0.189761           0.662787\n",
      "785                Left-Amygdala       0.209100           0.659783\n",
      "786            Right-Hippocampus       0.206738           0.661654\n",
      "787              ctx-lh-bankssts       0.180409           0.665927\n",
      "788   ctx-lh-caudalmiddlefrontal       0.160676           0.664392\n",
      "789            ctx-lh-entorhinal       0.191868           0.664203\n",
      "790              ctx-lh-fusiform       0.181073           0.665023\n",
      "791      ctx-lh-inferiortemporal       0.191604           0.668215\n",
      "792  ctx-lh-lateralorbitofrontal       0.186707           0.661238\n",
      "793        ctx-lh-middletemporal       0.190259           0.672436\n",
      "794       ctx-lh-parahippocampal       0.181642           0.665359\n",
      "795       ctx-lh-parsopercularis       0.168359           0.661411\n",
      "796  ctx-lh-rostralmiddlefrontal       0.168380           0.663602\n",
      "797      ctx-lh-superiortemporal       0.194380           0.666184\n",
      "798                ctx-lh-insula       0.188655           0.659730\n",
      "799              ctx-rh-bankssts       0.167619           0.678967\n",
      "800   ctx-rh-caudalmiddlefrontal       0.202815           0.666687\n",
      "801              ctx-rh-fusiform       0.196188           0.665695\n",
      "802      ctx-rh-inferiorparietal       0.185815           0.668835\n",
      "803      ctx-rh-inferiortemporal       0.211538           0.670535\n",
      "804  ctx-rh-lateralorbitofrontal       0.186869           0.662792\n",
      "805   ctx-rh-medialorbitofrontal       0.210135           0.660657\n",
      "806        ctx-rh-middletemporal       0.183773           0.675955\n",
      "807       ctx-rh-parahippocampal       0.199242           0.667038\n",
      "808           ctx-rh-postcentral       0.184831           0.662246\n",
      "809            ctx-rh-precentral       0.173697           0.663672\n",
      "810             ctx-rh-precuneus       0.152558           0.667911\n",
      "811  ctx-rh-rostralmiddlefrontal       0.174481           0.667550\n",
      "812       ctx-rh-superiorfrontal       0.187150           0.660569\n",
      "813      ctx-rh-superiorparietal       0.168886           0.671906\n",
      "814          ctx-rh-temporalpole       0.238526           0.666403\n",
      "815                ctx-rh-insula       0.170268           0.669684\n",
      "\n",
      "MNI Dataset Verification:\n",
      "-----------------------\n",
      "Total number of rows: 639\n",
      "Unique patients: 106\n",
      "Unique regions: 68\n",
      "\n",
      "Number of regions per MNI patient:\n",
      "count    106.000000\n",
      "mean       6.028302\n",
      "std        3.670883\n",
      "min        1.000000\n",
      "25%        3.000000\n",
      "50%        6.000000\n",
      "75%        8.000000\n",
      "max       16.000000\n",
      "Name: roi, dtype: float64\n",
      "\n",
      "Example - Hippocampus across HUP patients:\n",
      "      patient_id  deltaRel_mean  entropy_1min_mean\n",
      "82             7       0.203588           0.679797\n",
      "104            8       0.189688           0.662233\n",
      "176           12       0.205485           0.662669\n",
      "189           13       0.200127           0.676875\n",
      "224           15       0.204654           0.665854\n",
      "245           16       0.246618           0.672118\n",
      "260           17       0.194079           0.662270\n",
      "275           18       0.214286           0.662651\n",
      "300           19       0.216578           0.660658\n",
      "321           20       0.222828           0.664021\n",
      "347           22       0.197443           0.666198\n",
      "378           24       0.199521           0.663927\n",
      "400           26       0.201040           0.673291\n",
      "423           27       0.196466           0.678198\n",
      "448           28       0.198413           0.673589\n",
      "468           29       0.199002           0.662925\n",
      "508           32       0.190316           0.676630\n",
      "529           33       0.208037           0.659938\n",
      "553           35       0.203648           0.670231\n",
      "573           36       0.214173           0.670312\n",
      "613           39       0.216434           0.675415\n",
      "634           41       0.197903           0.662159\n",
      "650           42       0.215361           0.667289\n",
      "674           43       0.202516           0.664547\n",
      "722           45       0.215407           0.667116\n",
      "740           46       0.193883           0.674731\n",
      "762           47       0.268062           0.671531\n",
      "784           48       0.189761           0.662787\n",
      "817           49       0.254812           0.663861\n",
      "843           50       0.191449           0.662590\n",
      "890           52       0.228096           0.661882\n",
      "909           53       0.200882           0.674203\n",
      "938           54       0.204522           0.665272\n",
      "957           55       0.189676           0.660849\n",
      "972           56       0.213539           0.670258\n",
      "991           57       0.205905           0.664869\n",
      "1020          59       0.204287           0.660396\n",
      "1044          60       0.202155           0.662742\n",
      "\n",
      "Feature availability check:\n",
      "deltaRel_mean: 1064 non-null values\n",
      "thetaRel_mean: 1064 non-null values\n",
      "alphaRel_mean: 1064 non-null values\n",
      "betaRel_mean: 1064 non-null values\n",
      "gammaRel_mean: 1064 non-null values\n",
      "entropy_1min_mean: 1064 non-null values\n",
      "entropy_fullts_mean: 1064 non-null values\n"
     ]
    }
   ],
   "source": [
    "# Verify HUP data\n",
    "print(\"HUP Dataset Verification:\")\n",
    "print(\"-----------------------\")\n",
    "print(f\"Total number of rows: {len(hup_region_features)}\")\n",
    "print(f\"Unique patients: {len(hup_region_features['patient_id'].unique())}\")\n",
    "print(f\"Unique regions: {len(hup_region_features['roi'].unique())}\")\n",
    "\n",
    "# Count regions per patient\n",
    "hup_regions_per_patient = hup_region_features.groupby('patient_id')['roi'].count()\n",
    "print(\"\\nNumber of regions per HUP patient:\")\n",
    "print(hup_regions_per_patient.describe())\n",
    "print(\"\\nExample - Patient with most regions:\")\n",
    "max_patient = hup_regions_per_patient.idxmax()\n",
    "print(f\"\\nPatient {max_patient} regions:\")\n",
    "print(hup_region_features[hup_region_features['patient_id'] == max_patient][['roi', 'deltaRel_mean', 'entropy_1min_mean']])\n",
    "\n",
    "# Verify MNI data\n",
    "print(\"\\nMNI Dataset Verification:\")\n",
    "print(\"-----------------------\")\n",
    "print(f\"Total number of rows: {len(mni_region_features)}\")\n",
    "print(f\"Unique patients: {len(mni_region_features['patient_id'].unique())}\")\n",
    "print(f\"Unique regions: {len(mni_region_features['roi'].unique())}\")\n",
    "\n",
    "# Count regions per patient\n",
    "mni_regions_per_patient = mni_region_features.groupby('patient_id')['roi'].count()\n",
    "print(\"\\nNumber of regions per MNI patient:\")\n",
    "print(mni_regions_per_patient.describe())\n",
    "\n",
    "print(\"\\nExample - Hippocampus across HUP patients:\")\n",
    "hippocampus_data = hup_region_features[hup_region_features['roi'] == 'Left-Hippocampus']\n",
    "print(hippocampus_data[['patient_id', 'deltaRel_mean', 'entropy_1min_mean']])\n",
    "\n",
    "print(\"\\nFeature availability check:\")\n",
    "for column in hup_region_features.columns:\n",
    "    if column.endswith('_mean'):\n",
    "        non_null = hup_region_features[column].notna().sum()\n",
    "        print(f\"{column}: {non_null} non-null values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save combined features (region-wise)\n",
    "# hup_region_features.to_csv(os.path.join(base_path_results, 'hup_univar_feats_reg.csv'), index=False)\n",
    "# mni_region_features.to_csv(os.path.join(base_path_results, 'mni_univar_feats_reg.csv'), index=False)\n",
    "\n",
    "# Save electrode-level features\n",
    "hup_univar_feats.to_csv(os.path.join(base_path_results, 'hup_univar_feats_raw.csv'), index=True) \n",
    "mni_univar_feats.to_csv(os.path.join(base_path_results, 'mni_univar_feats_raw.csv'), index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hup_region_avg_feats.to_csv(os.path.join(base_path_results, 'hup_univar_feats_reg_avg.csv'), index=False) # already saved\n",
    "# mni_region_avg_feats.to_csv(os.path.join(base_path_results, 'mni_univar_feats_reg_avg.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((76, 10), (68, 10))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hup_region_avg_feats.shape, mni_region_avg_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1064, 10)\n",
      "(639, 10)\n"
     ]
    }
   ],
   "source": [
    "# print the dimensions of hup_region_features\n",
    "print(hup_region_features.shape)\n",
    "print(mni_region_features.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnt_gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
