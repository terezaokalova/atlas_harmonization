{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import (train_test_split, StratifiedKFold, \n",
    "                                   GridSearchCV, cross_val_score)\n",
    "from sklearn.metrics import (roc_curve, auc, confusion_matrix, \n",
    "                           classification_report)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, Tuple, List\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegionalMLAnalysis:\n",
    "    def __init__(self, region_analysis):\n",
    "        \"\"\"\n",
    "        Initialize with previously created RegionalAnalysis instance\n",
    "        \"\"\"\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.region_analysis = region_analysis\n",
    "        self.feature_columns = region_analysis.feature_columns\n",
    "        self.results = {}\n",
    "        \n",
    "    def prepare_region_data(self, region: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Prepare data for a specific region for ML analysis\n",
    "        \"\"\"\n",
    "        # Get data for specific region\n",
    "        hup_region = self.region_analysis.hup_features[\n",
    "            self.region_analysis.hup_features['roi'] == region\n",
    "        ]\n",
    "        mni_region = self.region_analysis.mni_features[\n",
    "            self.region_analysis.mni_features['roi'] == region\n",
    "        ]\n",
    "        \n",
    "        # Group by patient to get mean values\n",
    "        hup_data = hup_region.groupby('patient_id')[self.feature_columns].mean()\n",
    "        mni_data = mni_region.groupby('patient_id')[self.feature_columns].mean()\n",
    "        \n",
    "        # Check if enough samples\n",
    "        if len(hup_data) < 5 or len(mni_data) < 5:\n",
    "            raise ValueError(f\"Insufficient samples for region {region} \"\n",
    "                           f\"(HUP: {len(hup_data)}, MNI: {len(mni_data)})\")\n",
    "        \n",
    "        # Create feature matrix and labels\n",
    "        X = np.vstack([hup_data.values, mni_data.values])\n",
    "        y = np.hstack([np.ones(len(hup_data)), np.zeros(len(mni_data))])\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def train_evaluate_region(self, region: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Train and evaluate models for a specific region\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Prepare data\n",
    "            X, y = self.prepare_region_data(region)\n",
    "            \n",
    "            # Initialize results dictionary\n",
    "            region_results = {\n",
    "                'n_hup': sum(y == 1),\n",
    "                'n_mni': sum(y == 0),\n",
    "                'models': {}\n",
    "            }\n",
    "            \n",
    "            # Define models to try\n",
    "            models = {\n",
    "                'logistic': (LogisticRegression(), {\n",
    "                    'C': [0.1, 1.0, 10.0],\n",
    "                    'class_weight': ['balanced', None]\n",
    "                }),\n",
    "                'rf': (RandomForestClassifier(), {\n",
    "                    'n_estimators': [100, 200],\n",
    "                    'max_depth': [3, 5, None],\n",
    "                    'class_weight': ['balanced', None]\n",
    "                }),\n",
    "                'svm': (SVC(probability=True), {\n",
    "                    'C': [0.1, 1.0, 10.0],\n",
    "                    'kernel': ['linear', 'rbf'],\n",
    "                    'class_weight': ['balanced', None]\n",
    "                })\n",
    "            }\n",
    "            \n",
    "            # Scale features\n",
    "            scaler = StandardScaler()\n",
    "            X_scaled = scaler.fit_transform(X)\n",
    "            \n",
    "            # Cross-validation setup\n",
    "            cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            \n",
    "            # Train and evaluate each model type\n",
    "            for model_name, (model, param_grid) in models.items():\n",
    "                # Grid search\n",
    "                grid_search = GridSearchCV(\n",
    "                    model, param_grid,\n",
    "                    cv=cv, scoring='roc_auc',\n",
    "                    n_jobs=-1\n",
    "                )\n",
    "                \n",
    "                # Fit model\n",
    "                grid_search.fit(X_scaled, y)\n",
    "                \n",
    "                # Get cross-validation scores\n",
    "                cv_scores = cross_val_score(\n",
    "                    grid_search.best_estimator_,\n",
    "                    X_scaled, y,\n",
    "                    cv=cv,\n",
    "                    scoring='roc_auc'\n",
    "                )\n",
    "                \n",
    "                # Store results\n",
    "                model_results = {\n",
    "                    'best_params': grid_search.best_params_,\n",
    "                    'best_cv_score': grid_search.best_score_,\n",
    "                    'cv_scores_mean': cv_scores.mean(),\n",
    "                    'cv_scores_std': cv_scores.std()\n",
    "                }\n",
    "                \n",
    "                # Get feature importances for interpretable models\n",
    "                if model_name == 'logistic':\n",
    "                    importances = pd.Series(\n",
    "                        grid_search.best_estimator_.coef_[0],\n",
    "                        index=self.feature_columns\n",
    "                    ).sort_values(ascending=False)\n",
    "                    model_results['feature_importances'] = importances.to_dict()\n",
    "                elif model_name == 'rf':\n",
    "                    importances = pd.Series(\n",
    "                        grid_search.best_estimator_.feature_importances_,\n",
    "                        index=self.feature_columns\n",
    "                    ).sort_values(ascending=False)\n",
    "                    model_results['feature_importances'] = importances.to_dict()\n",
    "                \n",
    "                region_results['models'][model_name] = model_results\n",
    "            \n",
    "            return region_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Error processing region {region}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def analyze_all_regions(self):\n",
    "        \"\"\"\n",
    "        Perform ML analysis for all regions with sufficient data\n",
    "        \"\"\"\n",
    "        for region in self.region_analysis.common_regions:\n",
    "            self.logger.info(f\"Analyzing region: {region}\")\n",
    "            \n",
    "            results = self.train_evaluate_region(region)\n",
    "            if results is not None:\n",
    "                self.results[region] = results\n",
    "    \n",
    "    def summarize_results(self):\n",
    "        \"\"\"\n",
    "        Print summary of ML analysis results\n",
    "        \"\"\"\n",
    "        print(\"\\nRegional ML Analysis Summary\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Find top performing regions\n",
    "        region_performances = []\n",
    "        for region, results in self.results.items():\n",
    "            best_score = max(\n",
    "                model['cv_scores_mean'] \n",
    "                for model in results['models'].values()\n",
    "            )\n",
    "            best_model = max(\n",
    "                results['models'].items(),\n",
    "                key=lambda x: x[1]['cv_scores_mean']\n",
    "            )[0]\n",
    "            \n",
    "            region_performances.append({\n",
    "                'region': region,\n",
    "                'best_score': best_score,\n",
    "                'best_model': best_model,\n",
    "                'n_hup': results['n_hup'],\n",
    "                'n_mni': results['n_mni']\n",
    "            })\n",
    "        \n",
    "        # Sort by performance\n",
    "        region_performances.sort(key=lambda x: x['best_score'], reverse=True)\n",
    "        \n",
    "        # Print top 10 regions\n",
    "        print(\"\\nTop 10 regions by classification performance:\")\n",
    "        for i, perf in enumerate(region_performances[:10], 1):\n",
    "            print(f\"\\n{i}. {perf['region']}\")\n",
    "            print(f\"   Best model: {perf['best_model']}\")\n",
    "            print(f\"   ROC AUC: {perf['best_score']:.3f}\")\n",
    "            print(f\"   Samples: HUP={perf['n_hup']}, MNI={perf['n_mni']}\")\n",
    "            \n",
    "            # Print feature importances if available\n",
    "            region_results = self.results[perf['region']]\n",
    "            best_model_results = region_results['models'][perf['best_model']]\n",
    "            if 'feature_importances' in best_model_results:\n",
    "                print(\"   Top features:\")\n",
    "                importances = best_model_results['feature_importances']\n",
    "                for feature, importance in list(sorted(\n",
    "                    importances.items(), \n",
    "                    key=lambda x: abs(x[1]), \n",
    "                    reverse=True\n",
    "                ))[:3]:\n",
    "                    print(f\"    - {feature}: {importance:.3f}\")\n",
    "        \n",
    "        # Calculate overall statistics\n",
    "        print(\"\\nOverall Statistics:\")\n",
    "        print(f\"Total regions analyzed: {len(self.results)}\")\n",
    "        good_regions = sum(1 for perf in region_performances if perf['best_score'] > 0.7)\n",
    "        print(f\"Regions with ROC AUC > 0.7: {good_regions}\")\n",
    "        \n",
    "        # Model performance comparison\n",
    "        model_scores = {model: [] for model in ['logistic', 'rf', 'svm']}\n",
    "        for results in self.results.values():\n",
    "            for model_name, model_results in results['models'].items():\n",
    "                model_scores[model_name].append(model_results['cv_scores_mean'])\n",
    "        \n",
    "        print(\"\\nAverage performance by model type:\")\n",
    "        for model_name, scores in model_scores.items():\n",
    "            mean_score = np.mean(scores)\n",
    "            std_score = np.std(scores)\n",
    "            print(f\"{model_name}: {mean_score:.3f} ± {std_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnt_gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
