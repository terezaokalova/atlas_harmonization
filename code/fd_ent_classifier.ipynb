{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Directory: /Users/tereza/nishant/atlas/atlas_work_terez/atlas_harmonization/code\n",
      "Files available at results path:\n"
     ]
    }
   ],
   "source": [
    "code_directory = '/Users/tereza/nishant/atlas/atlas_work_terez/atlas_harmonization/code'\n",
    "os.chdir(code_directory)\n",
    "\n",
    "# set the paths relative to this new current directory\n",
    "base_path_data = '../Data'\n",
    "base_path_results = '../results'\n",
    "\n",
    "print(\"Current Directory:\", os.getcwd())\n",
    "\n",
    "print('Files available at results path:')\n",
    "files = os.listdir(base_path_results)\n",
    "# for file in files:\n",
    "    # print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path_results = '../results'\n",
    "figures_path = '../figures'\n",
    "results_path = '../results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "hup_region_features = pd.read_csv(os.path.join(results_path, 'ge_go_hup_region_features.csv'), index_col=0)\n",
    "mni_region_features = pd.read_csv(os.path.join(results_path, 'mni_region_features.csv'), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "106\n"
     ]
    }
   ],
   "source": [
    "# print(hup_region_features.head())\n",
    "# print(mni_region_features.head())\n",
    "\n",
    "hup_region_features.reset_index(inplace=True)\n",
    "mni_region_features.reset_index(inplace=True)\n",
    "print(hup_region_features['patient_id'].nunique())\n",
    "print(mni_region_features['patient_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_classification_data(hup_features: pd.DataFrame, \n",
    "#                               mni_features: pd.DataFrame,\n",
    "#                               significant_features: List[str] = None) -> Tuple[np.ndarray, np.ndarray]:\n",
    "#     \"\"\"\n",
    "#     Prepare data for site classification.\n",
    "    \n",
    "#     Args:\n",
    "#         hup_features: Region-level features for HUP patients\n",
    "#         mni_features: Region-level features for MNI patients\n",
    "#         significant_features: List of features to use (from statistical analysis)\n",
    "#     \"\"\"\n",
    "#     # Group by patient and compute mean across regions\n",
    "#     hup_patient_features = hup_features.groupby('patient_id')[significant_features].mean()\n",
    "#     mni_patient_features = mni_features.groupby('patient_id')[significant_features].mean()\n",
    "    \n",
    "#     # Create labels\n",
    "#     hup_labels = np.ones(len(hup_patient_features))\n",
    "#     mni_labels = np.zeros(len(mni_patient_features))\n",
    "    \n",
    "#     # Combine features and labels\n",
    "#     X = pd.concat([hup_patient_features, mni_patient_features])\n",
    "#     y = np.concatenate([hup_labels, mni_labels])\n",
    "    \n",
    "#     # Scale features\n",
    "#     scaler = StandardScaler()\n",
    "#     X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "#     return X_scaled, y\n",
    "\n",
    "# def perform_pca(X: np.ndarray, variance_threshold: float = 0.95) -> np.ndarray:\n",
    "#     \"\"\"\n",
    "#     Perform PCA while retaining specified variance\n",
    "#     \"\"\"\n",
    "#     pca = PCA()\n",
    "#     X_pca = pca.fit_transform(X)\n",
    "    \n",
    "#     # Calculate cumulative variance ratio\n",
    "#     cum_var_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "#     n_components = np.argmax(cum_var_ratio >= variance_threshold) + 1\n",
    "    \n",
    "#     return X_pca[:, :n_components]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique patients: 106\n"
     ]
    }
   ],
   "source": [
    "hup_region_features['site'] = 'HUP'\n",
    "mni_region_features['site'] = 'MNI'\n",
    "\n",
    "# Combine the datasets\n",
    "region_features = pd.concat([hup_region_features, mni_region_features], ignore_index=True)\n",
    "\n",
    "num_unique_patients = region_features['patient_id'].nunique() # doesn't work without reset_index\n",
    "print(f\"Number of unique patients: {num_unique_patients}\")\n",
    "\n",
    "# feature columns to aggregate\n",
    "feature_columns = ['deltaRel_mean', 'thetaRel_mean', 'alphaRel_mean', 'betaRel_mean', 'gammaRel_mean', 'entropy_1min_mean', 'entropy_fullts_mean']\n",
    "\n",
    "# aggregate features per patient\n",
    "patient_features = region_features.groupby(['patient_id', 'site'])[feature_columns].mean().reset_index() # doesn't work without reset_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnose_region_features(region_features: pd.DataFrame):\n",
    "    print(\"Dataset Overview:\")\n",
    "    print(f\"Total rows: {len(region_features)}\")\n",
    "    print(f\"Total columns: {region_features.columns.tolist()}\")\n",
    "    \n",
    "    print(\"\\nMissing Values:\")\n",
    "    print(region_features.isnull().sum())\n",
    "    \n",
    "    print(\"\\nPatients per site:\")\n",
    "    print(region_features.groupby('site')['patient_id'].nunique())\n",
    "    \n",
    "    print(\"\\nRegions per patient:\")\n",
    "    patient_region_counts = region_features.groupby(['site', 'patient_id']).size()\n",
    "    print(\"\\nSummary statistics of regions per patient:\")\n",
    "    print(patient_region_counts.groupby('site').describe())\n",
    "    \n",
    "    print(\"\\nSample of data structure:\")\n",
    "    print(region_features.groupby(['site', 'patient_id']).head(2))\n",
    "\n",
    "# diagnose_region_features(region_features)\n",
    "\n",
    "# Verify feature matrix after aggregation\n",
    "# print(\"\\nFeature matrix verification:\")\n",
    "# print(patient_features.groupby('site').size())\n",
    "# print(\"\\nFeature value ranges:\")\n",
    "# print(patient_features[feature_columns].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Patient-level feature matrix shape:\n",
      "(134, 9)\n",
      "\n",
      "Sample of aggregated features per patient:\n",
      "   patient_id site  deltaRel_mean  thetaRel_mean  alphaRel_mean  betaRel_mean  \\\n",
      "0           1  MNI       0.169199       0.150309       0.147994      0.175702   \n",
      "1           2  MNI       0.187945       0.207479       0.143997      0.133608   \n",
      "2           3  HUP       0.129081       0.122709       0.236676      0.179459   \n",
      "3           3  MNI       0.177230       0.179795       0.166074      0.151208   \n",
      "4           4  HUP       0.170417       0.182097       0.158259      0.159255   \n",
      "\n",
      "   gammaRel_mean  entropy_1min_mean  entropy_fullts_mean  \n",
      "0       0.124942           0.663203             0.792426  \n",
      "1       0.087666           0.667673             0.797214  \n",
      "2       0.057972           0.676374             0.806519  \n",
      "3       0.102935           0.663542             0.792790  \n",
      "4       0.114277           0.665870             0.795281  \n",
      "\n",
      "Verify unique patients:\n",
      "site\n",
      "HUP     28\n",
      "MNI    106\n",
      "Name: patient_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPatient-level feature matrix shape:\")\n",
    "print(patient_features.shape)\n",
    "print(\"\\nSample of aggregated features per patient:\")\n",
    "print(patient_features.head())\n",
    "print(\"\\nVerify unique patients:\")\n",
    "print(patient_features.groupby('site')['patient_id'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature matrix X\n",
    "X = patient_features[feature_columns]\n",
    "\n",
    "# Label vector y\n",
    "y = patient_features['site'].map({'HUP': 1, 'MNI': 0})\n",
    "\n",
    "# aggregate then standardize\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnt_gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
